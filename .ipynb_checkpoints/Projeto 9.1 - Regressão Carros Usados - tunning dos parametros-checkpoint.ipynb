{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856556ab-19b4-4ec2-82a9-c7575a3d75ed",
   "metadata": {},
   "source": [
    "# Projeto 9.1 - Regressão Carros Usados - Validação Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cad4be-89d3-49b0-b981-453cb9d1fe56",
   "metadata": {},
   "source": [
    "## 1. Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f11442c-605b-4360-a48e-03afc5023f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf0654-0dad-4877-a376-1cbaa5ae7f11",
   "metadata": {},
   "source": [
    "## 2. Importando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9694345-9e54-4652-b32d-56545b15abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1103d7f30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565f5503-70a5-463e-ba01-83dc0592000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv(\"data/autos.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e339293-2265-47c8-a6c2-5f9b2ae08bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateCrawled</th>\n",
       "      <th>name</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>nrOfPictures</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>lastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-24 11:52:17</td>\n",
       "      <td>Golf_3_1.6</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>70435</td>\n",
       "      <td>2016-04-07 03:16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-24 10:58:45</td>\n",
       "      <td>A5_Sportback_2.7_Tdi</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>66954</td>\n",
       "      <td>2016-04-07 01:46:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-14 12:52:21</td>\n",
       "      <td>Jeep_Grand_Cherokee_\"Overland\"</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>90480</td>\n",
       "      <td>2016-04-05 12:47:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-17 16:54:04</td>\n",
       "      <td>GOLF_4_1_4__3TÜRER</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91074</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-31 17:25:20</td>\n",
       "      <td>Skoda_Fabia_1.4_TDI_PD_Classic</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>60437</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dateCrawled                            name  seller offerType  \\\n",
       "0  2016-03-24 11:52:17                      Golf_3_1.6  privat   Angebot   \n",
       "1  2016-03-24 10:58:45            A5_Sportback_2.7_Tdi  privat   Angebot   \n",
       "2  2016-03-14 12:52:21  Jeep_Grand_Cherokee_\"Overland\"  privat   Angebot   \n",
       "3  2016-03-17 16:54:04              GOLF_4_1_4__3TÜRER  privat   Angebot   \n",
       "4  2016-03-31 17:25:20  Skoda_Fabia_1.4_TDI_PD_Classic  privat   Angebot   \n",
       "\n",
       "   price abtest vehicleType  yearOfRegistration    gearbox  powerPS  model  \\\n",
       "0    480   test         NaN                1993    manuell        0   golf   \n",
       "1  18300   test       coupe                2011    manuell      190    NaN   \n",
       "2   9800   test         suv                2004  automatik      163  grand   \n",
       "3   1500   test  kleinwagen                2001    manuell       75   golf   \n",
       "4   3600   test  kleinwagen                2008    manuell       69  fabia   \n",
       "\n",
       "   kilometer  monthOfRegistration fuelType       brand notRepairedDamage  \\\n",
       "0     150000                    0   benzin  volkswagen               NaN   \n",
       "1     125000                    5   diesel        audi                ja   \n",
       "2     125000                    8   diesel        jeep               NaN   \n",
       "3     150000                    6   benzin  volkswagen              nein   \n",
       "4      90000                    7   diesel       skoda              nein   \n",
       "\n",
       "           dateCreated  nrOfPictures  postalCode             lastSeen  \n",
       "0  2016-03-24 00:00:00             0       70435  2016-04-07 03:16:57  \n",
       "1  2016-03-24 00:00:00             0       66954  2016-04-07 01:46:50  \n",
       "2  2016-03-14 00:00:00             0       90480  2016-04-05 12:47:46  \n",
       "3  2016-03-17 00:00:00             0       91074  2016-03-17 17:40:17  \n",
       "4  2016-03-31 00:00:00             0       60437  2016-04-06 10:17:21  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9ff0bb-805c-4919-8ba6-896a66d4575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           480\n",
       "1         18300\n",
       "2          9800\n",
       "3          1500\n",
       "4          3600\n",
       "          ...  \n",
       "371523     2200\n",
       "371524     1199\n",
       "371525     9200\n",
       "371526     3400\n",
       "371527    28990\n",
       "Name: price, Length: 371528, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef75e42-8fd5-4975-ae35-f089166680c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dateCrawled', 'name', 'seller', 'offerType', 'price', 'abtest',\n",
       "       'vehicleType', 'yearOfRegistration', 'gearbox', 'powerPS', 'model',\n",
       "       'kilometer', 'monthOfRegistration', 'fuelType', 'brand',\n",
       "       'notRepairedDamage', 'dateCreated', 'nrOfPictures', 'postalCode',\n",
       "       'lastSeen'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e09da84-4678-4015-b5a8-47c4a01d3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.drop(columns=[\"dateCrawled\", \"dateCreated\", \"nrOfPictures\", \"postalCode\", \"lastSeen\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383658e3-00bd-429a-9a97-0f06a6609f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371528, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c822e338-e34c-4d73-af7f-85a4c40be1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golf_3_1.6</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A5_Sportback_2.7_Tdi</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeep_Grand_Cherokee_\"Overland\"</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOLF_4_1_4__3TÜRER</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skoda_Fabia_1.4_TDI_PD_Classic</td>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  seller offerType  price abtest vehicleType  \\\n",
       "0                      Golf_3_1.6  privat   Angebot    480   test         NaN   \n",
       "1            A5_Sportback_2.7_Tdi  privat   Angebot  18300   test       coupe   \n",
       "2  Jeep_Grand_Cherokee_\"Overland\"  privat   Angebot   9800   test         suv   \n",
       "3              GOLF_4_1_4__3TÜRER  privat   Angebot   1500   test  kleinwagen   \n",
       "4  Skoda_Fabia_1.4_TDI_PD_Classic  privat   Angebot   3600   test  kleinwagen   \n",
       "\n",
       "   yearOfRegistration    gearbox  powerPS  model  kilometer  \\\n",
       "0                1993    manuell        0   golf     150000   \n",
       "1                2011    manuell      190    NaN     125000   \n",
       "2                2004  automatik      163  grand     125000   \n",
       "3                2001    manuell       75   golf     150000   \n",
       "4                2008    manuell       69  fabia      90000   \n",
       "\n",
       "   monthOfRegistration fuelType       brand notRepairedDamage  \n",
       "0                    0   benzin  volkswagen               NaN  \n",
       "1                    5   diesel        audi                ja  \n",
       "2                    8   diesel        jeep               NaN  \n",
       "3                    6   benzin  volkswagen              nein  \n",
       "4                    7   diesel       skoda              nein  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f479fc9f-4ab4-42e6-a4ba-8dd31f3d0be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Ford_Fiesta                                              657\n",
       "BMW_318i                                                 627\n",
       "Opel_Corsa                                               622\n",
       "Volkswagen_Golf_1.4                                      603\n",
       "BMW_316i                                                 523\n",
       "                                                        ... \n",
       "Audi_A4_Avant_Klima_Gruene_Plakette_TÜV_&AU_NEU_XENON      1\n",
       "Renault_clio_in_gold_450VB_!!                              1\n",
       "Fiat_Doblo_1.6_Multijet                                    1\n",
       "Renault_Laguna_1                                           1\n",
       "BMW_M135i_vollausgestattet_NP_52.720____Euro               1\n",
       "Name: count, Length: 233531, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1586761e-cf1e-49c1-adaa-d2029eb3e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.drop(\"name\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be476bf5-626c-4a8b-b245-5a5ae3641565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>privat</td>\n",
       "      <td>Angebot</td>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seller offerType  price abtest vehicleType  yearOfRegistration    gearbox  \\\n",
       "0  privat   Angebot    480   test         NaN                1993    manuell   \n",
       "1  privat   Angebot  18300   test       coupe                2011    manuell   \n",
       "2  privat   Angebot   9800   test         suv                2004  automatik   \n",
       "3  privat   Angebot   1500   test  kleinwagen                2001    manuell   \n",
       "4  privat   Angebot   3600   test  kleinwagen                2008    manuell   \n",
       "\n",
       "   powerPS  model  kilometer  monthOfRegistration fuelType       brand  \\\n",
       "0        0   golf     150000                    0   benzin  volkswagen   \n",
       "1      190    NaN     125000                    5   diesel        audi   \n",
       "2      163  grand     125000                    8   diesel        jeep   \n",
       "3       75   golf     150000                    6   benzin  volkswagen   \n",
       "4       69  fabia      90000                    7   diesel       skoda   \n",
       "\n",
       "  notRepairedDamage  \n",
       "0               NaN  \n",
       "1                ja  \n",
       "2               NaN  \n",
       "3              nein  \n",
       "4              nein  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428a5927-2da1-4d62-aa12-0eff737ff203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seller\n",
       "privat        371525\n",
       "gewerblich         3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.seller.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4946f8ce-d833-45ac-a38c-efdf6ea1782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.drop(\"seller\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ccd51b-c2b1-401c-ab8f-8bd13dc1a5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offerType</th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Angebot</td>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angebot</td>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angebot</td>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angebot</td>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angebot</td>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  offerType  price abtest vehicleType  yearOfRegistration    gearbox  powerPS  \\\n",
       "0   Angebot    480   test         NaN                1993    manuell        0   \n",
       "1   Angebot  18300   test       coupe                2011    manuell      190   \n",
       "2   Angebot   9800   test         suv                2004  automatik      163   \n",
       "3   Angebot   1500   test  kleinwagen                2001    manuell       75   \n",
       "4   Angebot   3600   test  kleinwagen                2008    manuell       69   \n",
       "\n",
       "   model  kilometer  monthOfRegistration fuelType       brand  \\\n",
       "0   golf     150000                    0   benzin  volkswagen   \n",
       "1    NaN     125000                    5   diesel        audi   \n",
       "2  grand     125000                    8   diesel        jeep   \n",
       "3   golf     150000                    6   benzin  volkswagen   \n",
       "4  fabia      90000                    7   diesel       skoda   \n",
       "\n",
       "  notRepairedDamage  \n",
       "0               NaN  \n",
       "1                ja  \n",
       "2               NaN  \n",
       "3              nein  \n",
       "4              nein  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595fec14-2be2-4d2a-b77d-8542573be6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offerType\n",
       "Angebot    371516\n",
       "Gesuch         12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.offerType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "705a19c4-e49c-4128-9c97-04c524c80d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.drop(\"offerType\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d4edb6-2f0c-4682-8e6a-4c71a63ab3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>abtest</th>\n",
       "      <th>vehicleType</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerPS</th>\n",
       "      <th>model</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>monthOfRegistration</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>brand</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>manuell</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18300</td>\n",
       "      <td>test</td>\n",
       "      <td>coupe</td>\n",
       "      <td>2011</td>\n",
       "      <td>manuell</td>\n",
       "      <td>190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125000</td>\n",
       "      <td>5</td>\n",
       "      <td>diesel</td>\n",
       "      <td>audi</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9800</td>\n",
       "      <td>test</td>\n",
       "      <td>suv</td>\n",
       "      <td>2004</td>\n",
       "      <td>automatik</td>\n",
       "      <td>163</td>\n",
       "      <td>grand</td>\n",
       "      <td>125000</td>\n",
       "      <td>8</td>\n",
       "      <td>diesel</td>\n",
       "      <td>jeep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2001</td>\n",
       "      <td>manuell</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>6</td>\n",
       "      <td>benzin</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3600</td>\n",
       "      <td>test</td>\n",
       "      <td>kleinwagen</td>\n",
       "      <td>2008</td>\n",
       "      <td>manuell</td>\n",
       "      <td>69</td>\n",
       "      <td>fabia</td>\n",
       "      <td>90000</td>\n",
       "      <td>7</td>\n",
       "      <td>diesel</td>\n",
       "      <td>skoda</td>\n",
       "      <td>nein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price abtest vehicleType  yearOfRegistration    gearbox  powerPS  model  \\\n",
       "0    480   test         NaN                1993    manuell        0   golf   \n",
       "1  18300   test       coupe                2011    manuell      190    NaN   \n",
       "2   9800   test         suv                2004  automatik      163  grand   \n",
       "3   1500   test  kleinwagen                2001    manuell       75   golf   \n",
       "4   3600   test  kleinwagen                2008    manuell       69  fabia   \n",
       "\n",
       "   kilometer  monthOfRegistration fuelType       brand notRepairedDamage  \n",
       "0     150000                    0   benzin  volkswagen               NaN  \n",
       "1     125000                    5   diesel        audi                ja  \n",
       "2     125000                    8   diesel        jeep               NaN  \n",
       "3     150000                    6   benzin  volkswagen              nein  \n",
       "4      90000                    7   diesel       skoda              nein  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09f53e8d-df3d-41b4-bd4d-505497230ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12118, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = base.loc[base.price <= 10]\n",
    "i1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22bb8d42-b569-462f-ab52-76ca330c4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base[base.price > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87f33bdd-74cf-488f-8d36-82c80b1719a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359410, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32a986a2-def6-4ba4-b6cd-4eb43d85c244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2 = base.loc[base.price > 350000]\n",
    "i2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd376cec-a1aa-4929-b0a8-5f873294f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base[base.price < 350000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9cae7ae-b48a-481f-a8f5-a96a5d4152cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359291, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fa77d00-b22b-4046-a41e-b9fea7f8e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(base.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6799247e-ccbb-470c-994b-c02c075ca71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33546, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.loc[pd.isnull(base[\"vehicleType\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5085cc42-bd17-4d64-b050-def5bc3166d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vehicleType\n",
       "limousine     93614\n",
       "kleinwagen    78014\n",
       "kombi         65921\n",
       "bus           29699\n",
       "cabrio        22509\n",
       "coupe         18386\n",
       "suv           14477\n",
       "andere         3125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"vehicleType\"].value_counts() #limousine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b14e31f-9f07-4453-8400-20bf751c61a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17236, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.loc[pd.isnull(base[\"gearbox\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80bd3faa-1f12-418b-94ca-d1a01525b073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gearbox\n",
       "manuell      266547\n",
       "automatik     75508\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"gearbox\"].value_counts() #manuell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa63019-e4d0-42fc-a230-9da67606cc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17967, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.loc[pd.isnull(base[\"model\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d5d00de-f10c-4540-b5c8-2caaf3ebca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "golf               28989\n",
       "andere             25560\n",
       "3er                19905\n",
       "polo               12604\n",
       "corsa              12149\n",
       "                   ...  \n",
       "serie_2                8\n",
       "rangerover             6\n",
       "serie_3                3\n",
       "serie_1                1\n",
       "discovery_sport        1\n",
       "Name: count, Length: 251, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"model\"].value_counts() #golf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce026138-d962-4e46-b46e-4d791ad46ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29391, 12)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.loc[pd.isnull(base[\"fuelType\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "372eea55-9c98-4156-9a6f-03d775b19ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fuelType\n",
       "benzin     217582\n",
       "diesel     106002\n",
       "lpg          5222\n",
       "cng           557\n",
       "hybrid        271\n",
       "andere        165\n",
       "elektro       101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"fuelType\"].value_counts() #benzin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c643af54-5b51-45bc-a3a6-a061b11b6c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65986, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.loc[pd.isnull(base[\"notRepairedDamage\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "076c1f33-496a-4f7e-a141-0056c044e99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notRepairedDamage\n",
       "nein    259301\n",
       "ja       34004\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base[\"notRepairedDamage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b56d0c4f-86f2-4ac4-9b34-1d99c0c59c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = {\"vehicleType\": \"limousine\",\n",
    "           \"gearbox\": \"manuell\",\n",
    "           \"model\": \"golf\",\n",
    "           \"fuelType\": \"benzin\",\n",
    "           \"notRepairedDamage\": \"nein\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef5afc02-4a58-4382-826e-a33c7aeb219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base.fillna(value = valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7baac2b1-47e0-4fc4-be48-dff5084c34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price                  0\n",
       "abtest                 0\n",
       "vehicleType            0\n",
       "yearOfRegistration     0\n",
       "gearbox                0\n",
       "powerPS                0\n",
       "model                  0\n",
       "kilometer              0\n",
       "monthOfRegistration    0\n",
       "fuelType               0\n",
       "brand                  0\n",
       "notRepairedDamage      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eb040ad-79b9-4e2f-b6f1-e224089896da",
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_real = base[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "831b1e1a-120a-4096-a82c-2b9bf5f2d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = base.drop(columns=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d9d7a47-5f99-4dea-9f93-de8f7ec377a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abtest', 'vehicleType', 'yearOfRegistration', 'gearbox', 'powerPS',\n",
       "       'model', 'kilometer', 'monthOfRegistration', 'fuelType', 'brand',\n",
       "       'notRepairedDamage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecc3c726-fa6b-471e-82b1-424f80f0a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = ColumnTransformer(transformers=[(\"OneHot\", \n",
    "                                                 OneHotEncoder(), \n",
    "                                                 [0,1,3,5,8,9,10])],\n",
    "                                                 remainder=\"passthrough\"\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "987e0b75-a635-4e35-bcb3-dc611f3c0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = onehotencoder.fit_transform(previsores).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1922afb8-e64b-4484-868a-cb211555476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359291, 316)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73e05ee2-c088-4d6b-bb10-39793175515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = np.array(previsores)\n",
    "preco_real = np.array(preco_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b15f8d5-daae-407e-af5b-06874e03a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = previsores.astype('float32')\n",
    "preco_real = preco_real.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cef944-62b6-487b-8676-2a5c403fc5c6",
   "metadata": {},
   "source": [
    "## 3. Construção do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35374596-a16b-403e-af04-9ed87eefca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class regressao_torch(nn.Module):\n",
    "    def __init__(self, activation, neurons, initializer, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(316, neurons)\n",
    "        initializer(self.dense0.weight)\n",
    "        self.activation0 = activation\n",
    "        self.dropout0 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.dense1 = nn.Linear(neurons, neurons)\n",
    "        initializer(self.dense0.weight)\n",
    "        self.activation1 = activation\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.dense2 = nn.Linear(neurons, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dropout0(X)\n",
    "\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dropout1(X)\n",
    "\n",
    "        X = self.dense2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3939d416-5dfd-411b-bb6b-bfdb3c1b7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressao_sklearn = NeuralNetRegressor(module=regressao_torch,\n",
    "                                       optimizer__weight_decay=0.0001,\n",
    "                                       train_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946adb2d-d237-474c-ba4b-889d6f827ea5",
   "metadata": {},
   "source": [
    "## 4. Tuning de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06e36b16-58b5-4574-8e79-d5cdadd7af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': [300],\n",
    "    'max_epochs': [100],\n",
    "    'optimizer': [torch.optim.Adam],\n",
    "    'criterion': [torch.nn.L1Loss, torch.nn.MSELoss, torch.nn.SmoothL1Loss],\n",
    "    'module__activation': [F.relu],\n",
    "    'module__neurons': [158],\n",
    "    'module__initializer': [torch.nn.init.uniform_, torch.nn.init.normal_],\n",
    "    'module__dropout': [0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "935ec4fa-1a02-4946-a47c-d4f92573efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=regressao_sklearn,\n",
    "                          param_grid=params,\n",
    "                          scoring='neg_mean_absolute_error',\n",
    "                          cv=2,\n",
    "                          error_score=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1ad25e3-b1eb-4189-b525-6c3bc5e6517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5419.6074\u001b[0m  1.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4667.1914\u001b[0m  1.0262\n",
      "      3     \u001b[36m4666.6757\u001b[0m  1.0269\n",
      "      4     4667.8910  1.0244\n",
      "      5     \u001b[36m4661.4018\u001b[0m  1.0243\n",
      "      6     \u001b[36m4660.2143\u001b[0m  1.0303\n",
      "      7     \u001b[36m4658.1637\u001b[0m  1.0052\n",
      "      8     \u001b[36m4657.0017\u001b[0m  1.0093\n",
      "      9     \u001b[36m4655.4937\u001b[0m  1.0170\n",
      "     10     \u001b[36m4652.1609\u001b[0m  1.0620\n",
      "     11     \u001b[36m4651.1475\u001b[0m  1.0274\n",
      "     12     4653.0787  1.0036\n",
      "     13     \u001b[36m4650.2216\u001b[0m  1.0178\n",
      "     14     4650.3955  1.0124\n",
      "     15     4650.3735  1.0193\n",
      "     16     \u001b[36m4649.6103\u001b[0m  1.0172\n",
      "     17     \u001b[36m4649.0889\u001b[0m  1.0118\n",
      "     18     4649.8011  1.0254\n",
      "     19     \u001b[36m4647.3172\u001b[0m  1.0242\n",
      "     20     \u001b[36m4646.8778\u001b[0m  1.0185\n",
      "     21     4648.4620  1.0106\n",
      "     22     4647.4887  1.0200\n",
      "     23     \u001b[36m4634.0039\u001b[0m  1.0104\n",
      "     24     \u001b[36m4608.2196\u001b[0m  1.0168\n",
      "     25     \u001b[36m4605.7559\u001b[0m  1.0157\n",
      "     26     4606.7497  1.0165\n",
      "     27     \u001b[36m4605.6441\u001b[0m  1.0198\n",
      "     28     \u001b[36m4605.1485\u001b[0m  1.0159\n",
      "     29     \u001b[36m4604.7082\u001b[0m  1.0126\n",
      "     30     \u001b[36m4597.6836\u001b[0m  1.0257\n",
      "     31     \u001b[36m4586.0821\u001b[0m  1.0047\n",
      "     32     \u001b[36m4585.6810\u001b[0m  1.0158\n",
      "     33     \u001b[36m4585.5667\u001b[0m  1.0728\n",
      "     34     \u001b[36m4584.7582\u001b[0m  1.0173\n",
      "     35     \u001b[36m4584.1279\u001b[0m  1.0177\n",
      "     36     4585.7369  1.0276\n",
      "     37     4588.3259  1.0024\n",
      "     38     4584.4897  1.0295\n",
      "     39     4585.9266  1.0019\n",
      "     40     4584.3316  1.0165\n",
      "     41     4585.4431  1.0178\n",
      "     42     \u001b[36m4583.9403\u001b[0m  1.0325\n",
      "     43     4584.3241  1.0335\n",
      "     44     \u001b[36m4578.2031\u001b[0m  1.0557\n",
      "     45     \u001b[36m4576.1610\u001b[0m  1.0431\n",
      "     46     \u001b[36m4573.4847\u001b[0m  1.0135\n",
      "     47     \u001b[36m4572.9108\u001b[0m  1.1115\n",
      "     48     4573.8158  1.0422\n",
      "     49     \u001b[36m4571.7809\u001b[0m  1.0201\n",
      "     50     4574.3293  1.0320\n",
      "     51     4573.3029  0.9965\n",
      "     52     4573.4481  0.9920\n",
      "     53     4574.3172  1.0032\n",
      "     54     4576.1495  0.9981\n",
      "     55     4573.9324  1.0075\n",
      "     56     4573.3845  1.0127\n",
      "     57     4572.8526  1.0395\n",
      "     58     4573.1427  0.9956\n",
      "     59     4573.5099  1.0492\n",
      "     60     4574.9675  1.0563\n",
      "     61     4573.2084  0.9932\n",
      "     62     4573.9833  0.9939\n",
      "     63     4573.2991  1.0027\n",
      "     64     4573.5253  0.9968\n",
      "     65     4573.0460  0.9903\n",
      "     66     4572.6982  0.9807\n",
      "     67     \u001b[36m4570.8058\u001b[0m  1.0001\n",
      "     68     4573.3019  1.0440\n",
      "     69     4572.6955  1.0046\n",
      "     70     4573.0862  1.0054\n",
      "     71     4572.7084  1.0113\n",
      "     72     4573.8442  1.0009\n",
      "     73     4571.2823  1.0020\n",
      "     74     4572.2274  0.9938\n",
      "     75     4572.6892  0.9892\n",
      "     76     4572.1825  0.9780\n",
      "     77     4571.9137  0.9902\n",
      "     78     4572.0520  0.9937\n",
      "     79     4571.7024  0.9955\n",
      "     80     4571.5951  0.9867\n",
      "     81     4572.0188  0.9961\n",
      "     82     4571.6080  0.9920\n",
      "     83     4572.4890  0.9913\n",
      "     84     4573.0272  0.9738\n",
      "     85     4572.2185  0.9959\n",
      "     86     \u001b[36m4569.8477\u001b[0m  0.9825\n",
      "     87     4575.5276  0.9931\n",
      "     88     4571.9279  0.9918\n",
      "     89     4572.1830  1.0058\n",
      "     90     \u001b[36m4569.8066\u001b[0m  0.9977\n",
      "     91     \u001b[36m4569.7564\u001b[0m  0.9908\n",
      "     92     4573.4690  0.9973\n",
      "     93     \u001b[36m4569.6399\u001b[0m  0.9824\n",
      "     94     4572.2735  1.0042\n",
      "     95     4571.4041  1.0028\n",
      "     96     4571.0200  1.0021\n",
      "     97     4571.9854  1.0011\n",
      "     98     4572.6030  0.9937\n",
      "     99     \u001b[36m4569.4514\u001b[0m  1.0022\n",
      "    100     4571.5818  0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6166.5808\u001b[0m  1.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4904.5503\u001b[0m  1.0192\n",
      "      3     \u001b[36m4856.8977\u001b[0m  1.0088\n",
      "      4     \u001b[36m4850.3836\u001b[0m  1.0149\n",
      "      5     \u001b[36m4820.7822\u001b[0m  1.0162\n",
      "      6     \u001b[36m4702.2702\u001b[0m  1.0099\n",
      "      7     \u001b[36m4663.0047\u001b[0m  1.0234\n",
      "      8     \u001b[36m4658.1107\u001b[0m  1.0170\n",
      "      9     \u001b[36m4652.3621\u001b[0m  1.0112\n",
      "     10     4653.1934  1.0222\n",
      "     11     \u001b[36m4640.9027\u001b[0m  1.0139\n",
      "     12     \u001b[36m4624.6561\u001b[0m  1.0054\n",
      "     13     \u001b[36m4622.8714\u001b[0m  1.0093\n",
      "     14     4623.1232  1.0043\n",
      "     15     4623.0009  1.0077\n",
      "     16     4622.8720  1.0218\n",
      "     17     \u001b[36m4622.6789\u001b[0m  1.0035\n",
      "     18     \u001b[36m4622.0711\u001b[0m  1.0104\n",
      "     19     \u001b[36m4621.7041\u001b[0m  1.0078\n",
      "     20     \u001b[36m4621.2298\u001b[0m  1.0025\n",
      "     21     4621.7760  1.0124\n",
      "     22     4622.3219  1.0196\n",
      "     23     \u001b[36m4621.1386\u001b[0m  1.0097\n",
      "     24     \u001b[36m4620.5921\u001b[0m  1.0007\n",
      "     25     4621.9204  0.9961\n",
      "     26     4621.9470  1.0145\n",
      "     27     \u001b[36m4620.5433\u001b[0m  1.0629\n",
      "     28     4622.0232  1.0114\n",
      "     29     4621.3831  1.0065\n",
      "     30     \u001b[36m4620.1181\u001b[0m  1.0017\n",
      "     31     4620.3018  1.0085\n",
      "     32     4621.3580  1.0143\n",
      "     33     4620.2516  1.0238\n",
      "     34     \u001b[36m4619.4253\u001b[0m  1.0042\n",
      "     35     4620.3763  0.9998\n",
      "     36     4619.8286  0.9982\n",
      "     37     4620.6331  1.0118\n",
      "     38     4620.5976  0.9942\n",
      "     39     4622.4394  0.9870\n",
      "     40     4621.3624  1.0046\n",
      "     41     4624.8416  1.0637\n",
      "     42     \u001b[36m4617.9643\u001b[0m  1.0245\n",
      "     43     4620.6411  1.0078\n",
      "     44     4619.4941  1.0070\n",
      "     45     4621.0663  1.0060\n",
      "     46     4618.8976  1.0056\n",
      "     47     4619.7767  0.9961\n",
      "     48     4622.3415  1.0064\n",
      "     49     4619.5709  1.0021\n",
      "     50     4619.9389  1.0123\n",
      "     51     4618.9546  1.0147\n",
      "     52     \u001b[36m4615.6559\u001b[0m  1.0154\n",
      "     53     \u001b[36m4610.1644\u001b[0m  1.0897\n",
      "     54     4614.3309  1.0032\n",
      "     55     4619.0080  1.0093\n",
      "     56     4612.5261  1.0067\n",
      "     57     \u001b[36m4609.9742\u001b[0m  1.0584\n",
      "     58     4612.9938  1.0100\n",
      "     59     4612.7406  1.0025\n",
      "     60     4613.0322  1.0092\n",
      "     61     4612.1364  1.0286\n",
      "     62     4615.4066  1.0461\n",
      "     63     \u001b[36m4609.6257\u001b[0m  1.0625\n",
      "     64     4612.7820  1.1133\n",
      "     65     4614.5231  1.0636\n",
      "     66     4611.2852  1.0083\n",
      "     67     4612.9385  1.0133\n",
      "     68     4612.2770  1.0017\n",
      "     69     4612.1775  1.0011\n",
      "     70     4615.5482  1.0134\n",
      "     71     4611.9100  1.0020\n",
      "     72     4610.9477  0.9968\n",
      "     73     4611.5821  1.0037\n",
      "     74     4611.1908  1.0249\n",
      "     75     4611.6801  1.0259\n",
      "     76     4610.7346  1.0100\n",
      "     77     4611.2274  1.0170\n",
      "     78     4612.1080  1.0177\n",
      "     79     \u001b[36m4608.0849\u001b[0m  1.0039\n",
      "     80     4610.6719  0.9990\n",
      "     81     4609.4774  1.0180\n",
      "     82     4610.6411  1.0031\n",
      "     83     4611.4537  1.0152\n",
      "     84     4613.0663  1.0004\n",
      "     85     4612.8214  1.0060\n",
      "     86     4609.9570  1.0344\n",
      "     87     4610.1000  1.0130\n",
      "     88     4611.0226  1.0128\n",
      "     89     4610.9591  1.0118\n",
      "     90     4610.5386  1.0023\n",
      "     91     4610.8622  0.9935\n",
      "     92     4611.2693  0.9994\n",
      "     93     4610.6503  0.9902\n",
      "     94     4609.9340  1.0055\n",
      "     95     4611.1685  0.9985\n",
      "     96     4609.3389  1.0061\n",
      "     97     4608.9986  1.0112\n",
      "     98     4611.9201  1.0029\n",
      "     99     4609.8453  1.0167\n",
      "    100     4614.7472  1.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5497.5002\u001b[0m  1.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4601.5993\u001b[0m  1.0108\n",
      "      3     \u001b[36m4601.4222\u001b[0m  1.0028\n",
      "      4     4602.3503  1.0124\n",
      "      5     \u001b[36m4593.8258\u001b[0m  1.0040\n",
      "      6     \u001b[36m4593.6662\u001b[0m  1.0151\n",
      "      7     \u001b[36m4585.1195\u001b[0m  1.0096\n",
      "      8     \u001b[36m4583.9889\u001b[0m  1.0155\n",
      "      9     \u001b[36m4582.2947\u001b[0m  1.0030\n",
      "     10     \u001b[36m4579.0171\u001b[0m  0.9950\n",
      "     11     \u001b[36m4578.1798\u001b[0m  1.0137\n",
      "     12     4579.1607  1.0084\n",
      "     13     4578.2761  1.0036\n",
      "     14     \u001b[36m4577.6909\u001b[0m  1.0098\n",
      "     15     4579.9813  0.9955\n",
      "     16     4579.3083  1.0076\n",
      "     17     4578.9210  1.0079\n",
      "     18     4579.6034  1.0020\n",
      "     19     4579.5473  1.0175\n",
      "     20     \u001b[36m4577.2522\u001b[0m  1.0105\n",
      "     21     4578.9427  1.0072\n",
      "     22     \u001b[36m4575.8266\u001b[0m  1.0144\n",
      "     23     4579.6817  1.0042\n",
      "     24     4578.2838  1.0059\n",
      "     25     4577.4345  1.0024\n",
      "     26     4577.0743  1.0113\n",
      "     27     4577.5638  1.0109\n",
      "     28     4575.8921  1.0155\n",
      "     29     \u001b[36m4572.4489\u001b[0m  1.0164\n",
      "     30     4572.7835  1.0089\n",
      "     31     \u001b[36m4569.1745\u001b[0m  1.0716\n",
      "     32     4571.0272  1.0149\n",
      "     33     4570.4983  1.0038\n",
      "     34     4570.4385  1.0107\n",
      "     35     4570.2767  1.0206\n",
      "     36     \u001b[36m4569.0380\u001b[0m  1.0103\n",
      "     37     4571.2328  1.0183\n",
      "     38     4569.4967  1.0115\n",
      "     39     4569.0560  1.0107\n",
      "     40     4570.4723  1.0107\n",
      "     41     \u001b[36m4567.0463\u001b[0m  1.0049\n",
      "     42     4569.8727  1.0070\n",
      "     43     4567.7564  1.0059\n",
      "     44     4569.3847  1.0063\n",
      "     45     4568.6868  1.0367\n",
      "     46     4571.2291  0.9943\n",
      "     47     4569.1290  1.0044\n",
      "     48     4568.9273  0.9947\n",
      "     49     4569.3920  0.9973\n",
      "     50     4569.9366  0.9990\n",
      "     51     4569.6985  0.9994\n",
      "     52     4568.9924  1.0009\n",
      "     53     4568.6353  1.0129\n",
      "     54     4573.0650  1.0082\n",
      "     55     4567.6839  1.0036\n",
      "     56     4567.7537  1.0026\n",
      "     57     4569.9498  1.0083\n",
      "     58     4567.6165  1.0040\n",
      "     59     4567.4620  1.0064\n",
      "     60     4569.4181  1.0095\n",
      "     61     4568.6772  1.0081\n",
      "     62     4567.3470  1.0028\n",
      "     63     4572.1910  1.0032\n",
      "     64     4569.1158  1.0016\n",
      "     65     4570.9536  1.0094\n",
      "     66     4567.3884  1.0007\n",
      "     67     4569.5101  0.9955\n",
      "     68     4567.7735  0.9871\n",
      "     69     4572.1881  1.0235\n",
      "     70     4568.1466  1.0109\n",
      "     71     4568.6830  0.9959\n",
      "     72     4570.5761  1.0027\n",
      "     73     4568.2284  1.0106\n",
      "     74     4569.2647  1.0160\n",
      "     75     \u001b[36m4566.2039\u001b[0m  1.0035\n",
      "     76     4567.4459  1.0149\n",
      "     77     4567.6117  1.0141\n",
      "     78     4570.7333  1.0029\n",
      "     79     4568.3518  1.0395\n",
      "     80     4566.2817  1.0504\n",
      "     81     4568.8563  1.0141\n",
      "     82     4567.1829  1.0061\n",
      "     83     4567.6715  1.0044\n",
      "     84     4567.3560  1.0237\n",
      "     85     4568.0233  1.1732\n",
      "     86     4567.4104  1.1566\n",
      "     87     4567.2382  1.0267\n",
      "     88     4567.5888  1.1785\n",
      "     89     4568.3687  1.0335\n",
      "     90     4566.5889  0.9992\n",
      "     91     4568.1781  1.0190\n",
      "     92     4567.4123  1.0208\n",
      "     93     4567.3967  1.0163\n",
      "     94     4568.7468  1.0109\n",
      "     95     4567.4368  0.9987\n",
      "     96     \u001b[36m4565.5726\u001b[0m  0.9959\n",
      "     97     4567.2425  0.9927\n",
      "     98     4566.9559  1.0066\n",
      "     99     4566.5718  1.0062\n",
      "    100     4566.8717  1.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5500.8866\u001b[0m  1.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4635.4181\u001b[0m  1.0013\n",
      "      3     4642.3194  1.0522\n",
      "      4     4643.9725  1.0125\n",
      "      5     4641.1092  1.0007\n",
      "      6     \u001b[36m4635.2928\u001b[0m  0.9902\n",
      "      7     \u001b[36m4629.0086\u001b[0m  0.9989\n",
      "      8     \u001b[36m4622.4749\u001b[0m  0.9895\n",
      "      9     \u001b[36m4615.2999\u001b[0m  0.9936\n",
      "     10     \u001b[36m4605.3322\u001b[0m  0.9954\n",
      "     11     \u001b[36m4604.4318\u001b[0m  1.0019\n",
      "     12     \u001b[36m4602.6715\u001b[0m  1.0024\n",
      "     13     \u001b[36m4601.8442\u001b[0m  0.9971\n",
      "     14     \u001b[36m4601.3782\u001b[0m  0.9972\n",
      "     15     4606.2874  0.9975\n",
      "     16     4602.0033  0.9973\n",
      "     17     \u001b[36m4600.4927\u001b[0m  0.9930\n",
      "     18     4604.9028  1.0243\n",
      "     19     \u001b[36m4599.8034\u001b[0m  0.9925\n",
      "     20     4606.0766  1.0536\n",
      "     21     4605.8595  1.0178\n",
      "     22     4603.3396  1.0388\n",
      "     23     4603.3398  1.0359\n",
      "     24     4602.3394  1.0105\n",
      "     25     4603.9781  1.0085\n",
      "     26     4603.0863  1.0213\n",
      "     27     4603.3022  1.0111\n",
      "     28     4600.3696  1.0147\n",
      "     29     4601.7597  1.0059\n",
      "     30     4601.1796  1.0290\n",
      "     31     4605.0006  1.0504\n",
      "     32     4602.3726  1.0263\n",
      "     33     4601.6210  1.0112\n",
      "     34     4603.0355  1.0144\n",
      "     35     4602.7161  1.0149\n",
      "     36     4603.6457  1.0079\n",
      "     37     4601.8907  1.0175\n",
      "     38     4602.4688  1.0139\n",
      "     39     4600.4075  1.0256\n",
      "     40     4600.4898  1.0152\n",
      "     41     4602.8572  1.0095\n",
      "     42     4603.0457  1.0373\n",
      "     43     \u001b[36m4598.0225\u001b[0m  1.0173\n",
      "     44     4601.2830  1.0314\n",
      "     45     4600.8938  1.0146\n",
      "     46     4604.2801  1.0195\n",
      "     47     4601.9494  1.0180\n",
      "     48     4601.9133  1.0039\n",
      "     49     4605.7196  1.0547\n",
      "     50     4601.2522  1.0185\n",
      "     51     4603.7446  1.0351\n",
      "     52     4601.4989  1.0325\n",
      "     53     4603.7395  1.0651\n",
      "     54     4600.0941  1.1763\n",
      "     55     4602.2356  1.0450\n",
      "     56     4600.8152  1.0477\n",
      "     57     4600.8236  1.0222\n",
      "     58     4601.3611  1.0251\n",
      "     59     4600.0617  1.0270\n",
      "     60     4599.3539  1.0420\n",
      "     61     4601.5966  1.0174\n",
      "     62     4601.5654  1.0382\n",
      "     63     4602.2559  1.0292\n",
      "     64     4601.3023  1.0287\n",
      "     65     4600.8092  1.0215\n",
      "     66     4601.9076  1.0312\n",
      "     67     4601.3284  1.0239\n",
      "     68     4601.1404  1.0104\n",
      "     69     4601.3673  1.0202\n",
      "     70     4601.3305  1.0163\n",
      "     71     4601.4396  1.0089\n",
      "     72     4601.5397  1.0211\n",
      "     73     4601.6009  1.0172\n",
      "     74     4600.8171  1.0312\n",
      "     75     4598.7082  1.0431\n",
      "     76     4602.2624  1.0125\n",
      "     77     4601.9129  1.0495\n",
      "     78     4600.9678  1.0144\n",
      "     79     4601.8875  1.1035\n",
      "     80     4602.5620  1.0263\n",
      "     81     4599.1449  1.0294\n",
      "     82     4601.0763  1.0363\n",
      "     83     4602.1711  1.0184\n",
      "     84     4600.9912  1.0111\n",
      "     85     4599.5815  1.0155\n",
      "     86     4598.9479  1.0078\n",
      "     87     4602.0524  0.9922\n",
      "     88     4600.8822  1.0264\n",
      "     89     4599.6852  1.0221\n",
      "     90     4600.2496  1.0254\n",
      "     91     4601.6345  1.0161\n",
      "     92     4599.4033  1.0254\n",
      "     93     4598.5196  1.0224\n",
      "     94     4601.1621  1.0202\n",
      "     95     4600.0166  1.0376\n",
      "     96     4601.3552  1.0720\n",
      "     97     4598.1254  1.0460\n",
      "     98     4601.2758  1.0309\n",
      "     99     4600.5270  1.0143\n",
      "    100     4599.3329  1.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6016.7197\u001b[0m  1.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4972.4704\u001b[0m  1.0806\n",
      "      3     \u001b[36m4940.7255\u001b[0m  1.0411\n",
      "      4     \u001b[36m4939.2654\u001b[0m  1.0420\n",
      "      5     \u001b[36m4938.3634\u001b[0m  1.0395\n",
      "      6     \u001b[36m4934.8524\u001b[0m  1.0417\n",
      "      7     \u001b[36m4932.8037\u001b[0m  1.0502\n",
      "      8     \u001b[36m4928.5249\u001b[0m  1.0545\n",
      "      9     4929.3484  1.0273\n",
      "     10     \u001b[36m4919.7448\u001b[0m  1.0322\n",
      "     11     \u001b[36m4919.5815\u001b[0m  1.0418\n",
      "     12     \u001b[36m4917.4454\u001b[0m  1.0307\n",
      "     13     \u001b[36m4915.6384\u001b[0m  1.0342\n",
      "     14     \u001b[36m4881.3516\u001b[0m  1.0444\n",
      "     15     \u001b[36m4729.4094\u001b[0m  1.0423\n",
      "     16     4729.4929  1.0358\n",
      "     17     \u001b[36m4726.5506\u001b[0m  1.0236\n",
      "     18     \u001b[36m4726.1134\u001b[0m  1.0391\n",
      "     19     \u001b[36m4724.6683\u001b[0m  1.0819\n",
      "     20     \u001b[36m4722.5215\u001b[0m  1.0797\n",
      "     21     \u001b[36m4721.7318\u001b[0m  1.0300\n",
      "     22     4723.4278  1.0477\n",
      "     23     \u001b[36m4721.6879\u001b[0m  1.0426\n",
      "     24     \u001b[36m4720.7937\u001b[0m  1.0318\n",
      "     25     \u001b[36m4719.6916\u001b[0m  1.0087\n",
      "     26     \u001b[36m4661.6369\u001b[0m  1.0196\n",
      "     27     \u001b[36m4657.9352\u001b[0m  1.0377\n",
      "     28     \u001b[36m4656.1536\u001b[0m  1.0287\n",
      "     29     \u001b[36m4655.9193\u001b[0m  1.0448\n",
      "     30     \u001b[36m4654.9870\u001b[0m  1.0604\n",
      "     31     \u001b[36m4653.9445\u001b[0m  1.0466\n",
      "     32     \u001b[36m4652.5405\u001b[0m  1.0497\n",
      "     33     4655.2627  1.0533\n",
      "     34     4654.9375  1.0750\n",
      "     35     \u001b[36m4651.3815\u001b[0m  1.0605\n",
      "     36     4652.3054  1.0473\n",
      "     37     \u001b[36m4650.4891\u001b[0m  1.0397\n",
      "     38     4652.2679  1.0292\n",
      "     39     4652.1097  1.0439\n",
      "     40     \u001b[36m4617.8724\u001b[0m  1.0375\n",
      "     41     \u001b[36m4584.9001\u001b[0m  1.0127\n",
      "     42     \u001b[36m4581.5619\u001b[0m  1.0237\n",
      "     43     \u001b[36m4579.8284\u001b[0m  1.0268\n",
      "     44     \u001b[36m4578.3629\u001b[0m  1.0322\n",
      "     45     4581.5991  1.0392\n",
      "     46     4578.6734  1.0490\n",
      "     47     4578.6868  1.0283\n",
      "     48     4578.5591  1.0360\n",
      "     49     \u001b[36m4578.0963\u001b[0m  1.0352\n",
      "     50     4580.2759  1.0243\n",
      "     51     4578.3442  1.0235\n",
      "     52     \u001b[36m4577.9348\u001b[0m  1.0466\n",
      "     53     4579.4484  1.0230\n",
      "     54     4578.2398  1.0424\n",
      "     55     4578.8512  1.0478\n",
      "     56     \u001b[36m4576.8399\u001b[0m  1.0467\n",
      "     57     4578.0601  1.0425\n",
      "     58     4580.2352  1.0292\n",
      "     59     \u001b[36m4576.4465\u001b[0m  1.0282\n",
      "     60     4578.3120  1.0306\n",
      "     61     4577.7571  1.0442\n",
      "     62     4578.2435  1.0243\n",
      "     63     4578.5075  1.0260\n",
      "     64     4577.9890  1.0339\n",
      "     65     4578.8754  1.0206\n",
      "     66     4578.5467  1.0392\n",
      "     67     4577.0454  1.0385\n",
      "     68     4577.7762  1.0329\n",
      "     69     4576.5238  1.0309\n",
      "     70     4577.4604  1.0400\n",
      "     71     4577.1273  1.0196\n",
      "     72     4577.2177  1.0352\n",
      "     73     4576.9761  1.0462\n",
      "     74     \u001b[36m4575.5572\u001b[0m  1.0551\n",
      "     75     4576.8334  1.0287\n",
      "     76     4576.9197  1.0467\n",
      "     77     4577.6085  1.0918\n",
      "     78     4576.6032  1.0584\n",
      "     79     4576.7763  1.0170\n",
      "     80     4576.8729  1.0355\n",
      "     81     4576.8276  1.0357\n",
      "     82     4576.5952  1.0257\n",
      "     83     4576.1727  1.0341\n",
      "     84     4577.5072  1.0380\n",
      "     85     4576.2703  1.0401\n",
      "     86     4576.7721  1.0477\n",
      "     87     4575.9068  1.0561\n",
      "     88     4576.9312  1.0248\n",
      "     89     4577.0870  1.0395\n",
      "     90     \u001b[36m4574.8425\u001b[0m  1.0205\n",
      "     91     4577.0933  1.0334\n",
      "     92     4575.6280  1.0438\n",
      "     93     4575.3765  1.0112\n",
      "     94     4575.2420  1.0313\n",
      "     95     4575.8508  1.0445\n",
      "     96     4575.4951  1.0419\n",
      "     97     \u001b[36m4574.4516\u001b[0m  1.0250\n",
      "     98     4574.7068  1.0319\n",
      "     99     4576.0939  1.0428\n",
      "    100     4575.9815  1.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6946.5494\u001b[0m  1.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m5934.8175\u001b[0m  1.0573\n",
      "      3     \u001b[36m5928.8296\u001b[0m  1.0366\n",
      "      4     \u001b[36m5922.8464\u001b[0m  1.0169\n",
      "      5     \u001b[36m5916.8664\u001b[0m  1.0339\n",
      "      6     \u001b[36m5910.8893\u001b[0m  1.0379\n",
      "      7     \u001b[36m5904.9143\u001b[0m  1.0309\n",
      "      8     \u001b[36m5898.9407\u001b[0m  1.0274\n",
      "      9     \u001b[36m5892.9722\u001b[0m  1.0471\n",
      "     10     \u001b[36m5887.0180\u001b[0m  1.0841\n",
      "     11     \u001b[36m5881.0640\u001b[0m  1.0547\n",
      "     12     \u001b[36m5875.1097\u001b[0m  1.0332\n",
      "     13     \u001b[36m5869.1567\u001b[0m  1.0760\n",
      "     14     \u001b[36m5863.2064\u001b[0m  1.0218\n",
      "     15     \u001b[36m5857.2611\u001b[0m  1.0517\n",
      "     16     \u001b[36m5851.3158\u001b[0m  1.0635\n",
      "     17     \u001b[36m5845.3735\u001b[0m  1.0421\n",
      "     18     \u001b[36m5839.4773\u001b[0m  1.0534\n",
      "     19     \u001b[36m5833.5880\u001b[0m  1.0450\n",
      "     20     \u001b[36m5827.6924\u001b[0m  1.0370\n",
      "     21     \u001b[36m5821.7987\u001b[0m  1.1324\n",
      "     22     \u001b[36m5815.9112\u001b[0m  1.0363\n",
      "     23     \u001b[36m5810.0245\u001b[0m  1.0318\n",
      "     24     \u001b[36m5806.3724\u001b[0m  1.0212\n",
      "     25     \u001b[36m5798.2571\u001b[0m  1.0269\n",
      "     26     \u001b[36m5792.4065\u001b[0m  1.0313\n",
      "     27     \u001b[36m5785.7954\u001b[0m  1.0445\n",
      "     28     \u001b[36m4877.6948\u001b[0m  1.0457\n",
      "     29     \u001b[36m4738.6265\u001b[0m  1.0321\n",
      "     30     \u001b[36m4703.8304\u001b[0m  1.0376\n",
      "     31     \u001b[36m4701.1346\u001b[0m  1.0335\n",
      "     32     4702.9522  1.0553\n",
      "     33     \u001b[36m4700.9308\u001b[0m  1.0892\n",
      "     34     \u001b[36m4680.9375\u001b[0m  1.0804\n",
      "     35     \u001b[36m4671.7196\u001b[0m  1.0444\n",
      "     36     4672.1458  1.0455\n",
      "     37     \u001b[36m4670.2853\u001b[0m  1.0376\n",
      "     38     4671.2272  1.0543\n",
      "     39     4670.9125  1.0333\n",
      "     40     4673.2940  1.0207\n",
      "     41     \u001b[36m4652.5303\u001b[0m  1.0210\n",
      "     42     \u001b[36m4639.7784\u001b[0m  1.0395\n",
      "     43     4645.4405  1.0290\n",
      "     44     \u001b[36m4638.0860\u001b[0m  1.1475\n",
      "     45     4641.9001  1.0409\n",
      "     46     4639.7730  1.0401\n",
      "     47     4640.5867  1.0437\n",
      "     48     4638.2677  1.0472\n",
      "     49     4639.8602  1.0321\n",
      "     50     4639.1185  1.0546\n",
      "     51     4639.2469  1.0280\n",
      "     52     \u001b[36m4633.4493\u001b[0m  1.0307\n",
      "     53     \u001b[36m4631.5799\u001b[0m  1.0288\n",
      "     54     \u001b[36m4631.0815\u001b[0m  1.0090\n",
      "     55     4631.4224  1.0263\n",
      "     56     \u001b[36m4630.1451\u001b[0m  1.0295\n",
      "     57     4630.2139  1.0519\n",
      "     58     \u001b[36m4628.7213\u001b[0m  1.0388\n",
      "     59     \u001b[36m4628.1009\u001b[0m  1.0408\n",
      "     60     4630.9691  1.0335\n",
      "     61     4629.4893  1.0522\n",
      "     62     4632.1660  1.0395\n",
      "     63     4629.9772  1.0335\n",
      "     64     4629.8696  1.0316\n",
      "     65     \u001b[36m4625.1723\u001b[0m  1.0424\n",
      "     66     \u001b[36m4625.1178\u001b[0m  1.0345\n",
      "     67     4625.1793  1.0600\n",
      "     68     \u001b[36m4624.7538\u001b[0m  1.0564\n",
      "     69     \u001b[36m4623.0025\u001b[0m  1.0327\n",
      "     70     4624.0036  1.0435\n",
      "     71     4625.3964  1.0295\n",
      "     72     4623.0848  1.0413\n",
      "     73     \u001b[36m4621.5332\u001b[0m  1.0289\n",
      "     74     4622.6911  1.0199\n",
      "     75     4624.5412  1.0229\n",
      "     76     4623.1420  1.0236\n",
      "     77     4622.1046  1.0542\n",
      "     78     4624.4648  1.0640\n",
      "     79     4624.1235  1.0144\n",
      "     80     4622.6199  1.0315\n",
      "     81     4623.6442  1.3232\n",
      "     82     4624.6429  1.0759\n",
      "     83     4624.3830  1.0585\n",
      "     84     4623.6096  1.0458\n",
      "     85     4622.8424  1.0433\n",
      "     86     4624.1285  1.0544\n",
      "     87     \u001b[36m4620.2981\u001b[0m  1.0499\n",
      "     88     4623.7345  1.0674\n",
      "     89     4623.2273  1.0468\n",
      "     90     4621.6465  1.0298\n",
      "     91     4624.6077  1.0689\n",
      "     92     4621.9642  1.0235\n",
      "     93     4621.3019  1.0439\n",
      "     94     4624.4013  1.0386\n",
      "     95     4623.6969  1.0345\n",
      "     96     4622.5079  1.0547\n",
      "     97     4622.0684  1.0406\n",
      "     98     4624.2373  1.0501\n",
      "     99     4623.9807  1.0552\n",
      "    100     4621.6966  1.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5115.4620\u001b[0m  1.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4616.8859\u001b[0m  1.0299\n",
      "      3     \u001b[36m4608.9372\u001b[0m  1.0489\n",
      "      4     \u001b[36m4603.6152\u001b[0m  1.0244\n",
      "      5     \u001b[36m4589.4958\u001b[0m  1.0574\n",
      "      6     \u001b[36m4583.9952\u001b[0m  1.0709\n",
      "      7     \u001b[36m4579.0818\u001b[0m  1.0522\n",
      "      8     4587.0376  1.0348\n",
      "      9     4582.0924  1.0425\n",
      "     10     4581.9173  1.0268\n",
      "     11     4584.0266  1.0163\n",
      "     12     4581.5571  1.0308\n",
      "     13     4579.4056  1.0252\n",
      "     14     4579.5422  1.0381\n",
      "     15     \u001b[36m4577.8722\u001b[0m  1.0522\n",
      "     16     4578.8023  1.0306\n",
      "     17     4579.4580  1.0171\n",
      "     18     4579.4169  1.0529\n",
      "     19     4579.2429  1.0682\n",
      "     20     4578.9991  1.0355\n",
      "     21     \u001b[36m4576.0276\u001b[0m  1.0410\n",
      "     22     4576.1563  1.0229\n",
      "     23     \u001b[36m4575.6747\u001b[0m  1.0266\n",
      "     24     4575.8959  1.0392\n",
      "     25     \u001b[36m4575.2166\u001b[0m  1.0303\n",
      "     26     4576.5654  1.0507\n",
      "     27     4575.6061  1.0324\n",
      "     28     4576.8042  1.0205\n",
      "     29     4577.5645  1.0118\n",
      "     30     4576.7281  1.0264\n",
      "     31     4577.2503  1.0164\n",
      "     32     4576.8703  1.0312\n",
      "     33     4575.4999  1.0318\n",
      "     34     4575.3635  1.0443\n",
      "     35     \u001b[36m4575.1489\u001b[0m  1.0379\n",
      "     36     4577.4808  1.0629\n",
      "     37     4576.9823  1.0352\n",
      "     38     4576.3922  1.0429\n",
      "     39     4575.4820  1.0416\n",
      "     40     4576.1023  1.1666\n",
      "     41     4576.0872  1.0299\n",
      "     42     \u001b[36m4574.9664\u001b[0m  1.0148\n",
      "     43     \u001b[36m4574.4500\u001b[0m  1.0246\n",
      "     44     4577.5586  1.0308\n",
      "     45     \u001b[36m4574.3454\u001b[0m  1.0181\n",
      "     46     4576.4310  1.1848\n",
      "     47     4576.5289  1.0293\n",
      "     48     \u001b[36m4574.0046\u001b[0m  1.0745\n",
      "     49     4574.5940  1.0224\n",
      "     50     \u001b[36m4573.2174\u001b[0m  1.0261\n",
      "     51     4574.6240  1.0239\n",
      "     52     4575.4387  1.0140\n",
      "     53     4575.2525  1.0524\n",
      "     54     4577.9648  1.0481\n",
      "     55     4578.1094  1.0172\n",
      "     56     4578.1414  1.0116\n",
      "     57     4579.2745  1.0174\n",
      "     58     4577.0387  1.0194\n",
      "     59     4578.0717  1.0469\n",
      "     60     4575.2757  1.0155\n",
      "     61     4573.4736  1.0049\n",
      "     62     4574.2527  1.0240\n",
      "     63     4574.1184  1.0323\n",
      "     64     4574.8765  926.2620\n",
      "     65     4574.5611  1.4375\n",
      "     66     4577.9638  1.0753\n",
      "     67     4575.4198  1.3530\n",
      "     68     4574.3635  1.3457\n",
      "     69     4573.4279  1.2689\n",
      "     70     \u001b[36m4570.9831\u001b[0m  1.1126\n",
      "     71     4575.5197  1.0710\n",
      "     72     4574.3779  1.0240\n",
      "     73     4573.3631  1.0992\n",
      "     74     4573.6483  1.0636\n",
      "     75     4574.0540  1.0805\n",
      "     76     4574.1909  1.1068\n",
      "     77     \u001b[36m4569.0314\u001b[0m  1.0420\n",
      "     78     4570.1051  1.0581\n",
      "     79     4570.9173  1.1073\n",
      "     80     \u001b[36m4568.3254\u001b[0m  1.0312\n",
      "     81     \u001b[36m4567.6635\u001b[0m  1.0261\n",
      "     82     4567.7685  1.0436\n",
      "     83     \u001b[36m4567.0840\u001b[0m  1.0524\n",
      "     84     \u001b[36m4566.4063\u001b[0m  1.0464\n",
      "     85     4566.4892  1.0650\n",
      "     86     \u001b[36m4566.1193\u001b[0m  1.0390\n",
      "     87     4566.5744  1.0229\n",
      "     88     \u001b[36m4564.8807\u001b[0m  1.0453\n",
      "     89     4567.8602  1.0253\n",
      "     90     4565.7260  1.0332\n",
      "     91     4566.9726  1.0239\n",
      "     92     4566.5126  1.0429\n",
      "     93     4565.9392  1.0498\n",
      "     94     4565.5081  1.0443\n",
      "     95     4566.9226  1.0545\n",
      "     96     4565.0943  1.0251\n",
      "     97     4566.4273  1.0400\n",
      "     98     4565.3830  1.0591\n",
      "     99     4566.6488  1.0374\n",
      "    100     4566.6385  1.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5566.3132\u001b[0m  1.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4654.2292\u001b[0m  1.0354\n",
      "      3     4666.5522  1.1082\n",
      "      4     4663.3593  1.0446\n",
      "      5     \u001b[36m4654.1255\u001b[0m  1.0444\n",
      "      6     \u001b[36m4643.9796\u001b[0m  1.0761\n",
      "      7     \u001b[36m4628.9475\u001b[0m  926.4093\n",
      "      8     \u001b[36m4619.2589\u001b[0m  1.0834\n",
      "      9     \u001b[36m4615.4822\u001b[0m  1.0679\n",
      "     10     4622.5065  1.0733\n",
      "     11     4623.5084  1.0610\n",
      "     12     4621.2174  1.0392\n",
      "     13     4616.1618  1.0495\n",
      "     14     \u001b[36m4612.7385\u001b[0m  1.0308\n",
      "     15     4615.6931  1.0231\n",
      "     16     4619.7006  1.0326\n",
      "     17     4614.8589  1.0233\n",
      "     18     4621.4629  1.0607\n",
      "     19     4616.6659  1.0433\n",
      "     20     4617.1365  1.0358\n",
      "     21     4617.8225  1.0350\n",
      "     22     4623.9153  1.0620\n",
      "     23     4623.5816  1.0512\n",
      "     24     4621.9930  1.0324\n",
      "     25     4621.6280  1.0367\n",
      "     26     4619.4356  1.0398\n",
      "     27     4622.7384  1.0388\n",
      "     28     4620.2734  1.0379\n",
      "     29     4621.1396  1.0415\n",
      "     30     4621.1142  1.0495\n",
      "     31     4620.9017  1.0371\n",
      "     32     4620.4627  1.0492\n",
      "     33     4621.6456  1.0289\n",
      "     34     4621.5014  1.0350\n",
      "     35     4621.3811  1.0357\n",
      "     36     4621.0649  1.0387\n",
      "     37     4620.4144  1.0366\n",
      "     38     4619.9064  1.0470\n",
      "     39     4620.4400  1.0354\n",
      "     40     4621.3916  1.0337\n",
      "     41     4618.3701  1.0242\n",
      "     42     4619.4831  1.0296\n",
      "     43     4621.1359  1.0483\n",
      "     44     4620.8488  1.0444\n",
      "     45     4619.9843  1.0193\n",
      "     46     4620.9416  1.0363\n",
      "     47     4621.2128  1.0198\n",
      "     48     4620.8660  1.0448\n",
      "     49     4619.0514  1.0403\n",
      "     50     4622.6379  1.0385\n",
      "     51     4621.2562  1.0332\n",
      "     52     4619.1903  1.0580\n",
      "     53     4619.3153  1.0514\n",
      "     54     4617.1408  1.0413\n",
      "     55     4619.7382  1.0242\n",
      "     56     4620.1285  1.0353\n",
      "     57     4618.5478  1.0546\n",
      "     58     4617.4471  1.0502\n",
      "     59     4619.8956  1.0319\n",
      "     60     4620.6580  1.0616\n",
      "     61     4620.8321  1.0676\n",
      "     62     4618.1986  1.0625\n",
      "     63     4619.6497  1.0703\n",
      "     64     4619.9389  1.0748\n",
      "     65     4619.1223  1.0930\n",
      "     66     4619.2436  1.0658\n",
      "     67     4624.4382  926.6732\n",
      "     68     4618.3537  1.5228\n",
      "     69     4618.6348  1.3396\n",
      "     70     4622.1734  1.1239\n",
      "     71     4621.7833  1.0618\n",
      "     72     4618.8226  1.0825\n",
      "     73     4618.2720  1.0581\n",
      "     74     4618.6607  1.0155\n",
      "     75     4619.1291  1.0306\n",
      "     76     4618.7048  1.0203\n",
      "     77     4619.9820  1.0375\n",
      "     78     4617.5645  1.0455\n",
      "     79     4617.4308  1.1546\n",
      "     80     4618.5371  1.0253\n",
      "     81     4618.0748  1.0349\n",
      "     82     4617.3529  1.0317\n",
      "     83     4619.8161  1.0292\n",
      "     84     4617.4755  1.0251\n",
      "     85     4619.2983  1.0511\n",
      "     86     4619.0468  1.0180\n",
      "     87     4616.3324  1.0221\n",
      "     88     4619.4700  1.0233\n",
      "     89     4618.7295  1.0264\n",
      "     90     4619.8536  1.0086\n",
      "     91     4617.8527  1.0386\n",
      "     92     4615.6245  1.0228\n",
      "     93     4617.4200  1.0397\n",
      "     94     4617.1106  1.0498\n",
      "     95     4616.1290  1.0526\n",
      "     96     4617.7639  1.0167\n",
      "     97     4616.3767  1.1121\n",
      "     98     4614.4202  1.0348\n",
      "     99     4614.5256  1.0337\n",
      "    100     4615.7996  1.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m351017941.0603\u001b[0m  0.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m82960585.1725\u001b[0m  0.9753\n",
      "      3  \u001b[36m82949869.9287\u001b[0m  0.9850\n",
      "      4  \u001b[36m82948895.8957\u001b[0m  0.9883\n",
      "      5  \u001b[36m82899777.5103\u001b[0m  0.9808\n",
      "      6  \u001b[36m82728359.7611\u001b[0m  0.9842\n",
      "      7  \u001b[36m82204333.7037\u001b[0m  0.9735\n",
      "      8  \u001b[36m81081728.4222\u001b[0m  1.0061\n",
      "      9  \u001b[36m80765416.7751\u001b[0m  0.9707\n",
      "     10  80802296.9257  926.2113\n",
      "     11  \u001b[36m80754723.5399\u001b[0m  1.0660\n",
      "     12  80823845.1604  0.9655\n",
      "     13  80868828.6224  0.9701\n",
      "     14  80837743.4239  0.9651\n",
      "     15  \u001b[36m80735313.2019\u001b[0m  0.9668\n",
      "     16  \u001b[36m80676945.3066\u001b[0m  1.0290\n",
      "     17  \u001b[36m80675802.4159\u001b[0m  0.9666\n",
      "     18  \u001b[36m80516787.0679\u001b[0m  0.9591\n",
      "     19  \u001b[36m80481259.6734\u001b[0m  0.9658\n",
      "     20  \u001b[36m80460044.6189\u001b[0m  0.9580\n",
      "     21  \u001b[36m80402993.2618\u001b[0m  0.9623\n",
      "     22  80469816.0269  0.9773\n",
      "     23  80470202.7435  0.9507\n",
      "     24  80457714.4672  0.9626\n",
      "     25  80422810.9778  0.9659\n",
      "     26  80403186.1821  0.9678\n",
      "     27  80406202.4487  0.9681\n",
      "     28  \u001b[36m80402647.1041\u001b[0m  0.9702\n",
      "     29  \u001b[36m80362213.2824\u001b[0m  0.9492\n",
      "     30  80389764.8739  0.9628\n",
      "     31  \u001b[36m80359018.9706\u001b[0m  0.9689\n",
      "     32  \u001b[36m80352482.2284\u001b[0m  0.9711\n",
      "     33  \u001b[36m80340261.8305\u001b[0m  0.9691\n",
      "     34  80382613.0226  0.9717\n",
      "     35  80360572.4612  0.9548\n",
      "     36  \u001b[36m80305457.6014\u001b[0m  0.9672\n",
      "     37  80336630.0812  0.9514\n",
      "     38  \u001b[36m80305037.6590\u001b[0m  0.9619\n",
      "     39  80305814.5374  1.0305\n",
      "     40  \u001b[36m80288499.8320\u001b[0m  1.1068\n",
      "     41  \u001b[36m80279218.1058\u001b[0m  0.9995\n",
      "     42  80296577.3286  0.9674\n",
      "     43  80291963.0766  0.9970\n",
      "     44  80306143.5482  0.9878\n",
      "     45  \u001b[36m80262083.0008\u001b[0m  0.9665\n",
      "     46  \u001b[36m80260049.0237\u001b[0m  0.9664\n",
      "     47  \u001b[36m80246511.5553\u001b[0m  0.9885\n",
      "     48  80261981.9005  0.9784\n",
      "     49  80274254.2627  1.0142\n",
      "     50  \u001b[36m80220908.3206\u001b[0m  0.9678\n",
      "     51  \u001b[36m80212694.4370\u001b[0m  0.9493\n",
      "     52  \u001b[36m80208362.5736\u001b[0m  0.9696\n",
      "     53  \u001b[36m80208170.5837\u001b[0m  0.9603\n",
      "     54  80254239.3077  0.9775\n",
      "     55  \u001b[36m80192920.7448\u001b[0m  0.9701\n",
      "     56  80195854.4634  0.9581\n",
      "     57  80238777.4583  0.9771\n",
      "     58  \u001b[36m79757060.9537\u001b[0m  926.2064\n",
      "     59  \u001b[36m79197974.2364\u001b[0m  1.3938\n",
      "     60  \u001b[36m79179251.2315\u001b[0m  0.9642\n",
      "     61  \u001b[36m79109704.5843\u001b[0m  0.9620\n",
      "     62  79118656.6213  0.9717\n",
      "     63  79275956.3804  0.9617\n",
      "     64  79160379.6216  0.9655\n",
      "     65  79144868.6120  0.9603\n",
      "     66  79123886.7366  0.9623\n",
      "     67  \u001b[36m78894216.3679\u001b[0m  0.9632\n",
      "     68  \u001b[36m78648715.7069\u001b[0m  0.9679\n",
      "     69  \u001b[36m78603093.1025\u001b[0m  0.9679\n",
      "     70  \u001b[36m78599701.7709\u001b[0m  0.9650\n",
      "     71  \u001b[36m78594682.0227\u001b[0m  0.9551\n",
      "     72  \u001b[36m78565280.9869\u001b[0m  0.9621\n",
      "     73  78603998.6051  0.9615\n",
      "     74  78595329.5773  0.9624\n",
      "     75  78615255.3533  0.9490\n",
      "     76  78572096.8039  0.9727\n",
      "     77  78579351.0216  1.0297\n",
      "     78  78582597.0672  1.0552\n",
      "     79  78570636.0794  1.0276\n",
      "     80  78592291.5659  0.9613\n",
      "     81  78588648.9091  0.9796\n",
      "     82  78568155.5353  0.9726\n",
      "     83  78613110.2628  1.0117\n",
      "     84  78579740.6306  0.9805\n",
      "     85  78732256.4609  0.9648\n",
      "     86  78602505.9634  0.9591\n",
      "     87  \u001b[36m78533860.4902\u001b[0m  0.9735\n",
      "     88  78651327.2586  0.9783\n",
      "     89  \u001b[36m78475807.4303\u001b[0m  1.0417\n",
      "     90  \u001b[36m78335039.0721\u001b[0m  0.9861\n",
      "     91  \u001b[36m78267438.2422\u001b[0m  1.1285\n",
      "     92  78362442.4868  1.1532\n",
      "     93  \u001b[36m78235206.0062\u001b[0m  1.0708\n",
      "     94  78398421.3899  1.0502\n",
      "     95  78379609.6639  0.9714\n",
      "     96  \u001b[36m78214796.9749\u001b[0m  0.9861\n",
      "     97  78308847.3496  0.9630\n",
      "     98  \u001b[36m78214705.5564\u001b[0m  0.9699\n",
      "     99  78261447.2698  0.9535\n",
      "    100  78241067.7242  0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m295681024.0102\u001b[0m  0.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m85821977.4497\u001b[0m  1.0153\n",
      "      3  \u001b[36m85707498.3700\u001b[0m  0.9816\n",
      "      4  \u001b[36m84441983.7352\u001b[0m  0.9696\n",
      "      5  \u001b[36m83756804.7845\u001b[0m  926.4311\n",
      "      6  \u001b[36m83541086.0848\u001b[0m  0.9642\n",
      "      7  83567827.5070  0.9708\n",
      "      8  83653917.1012  0.9565\n",
      "      9  83723576.1893  0.9760\n",
      "     10  83724669.3996  0.9665\n",
      "     11  83694025.4597  0.9586\n",
      "     12  83785291.3288  0.9718\n",
      "     13  83731611.6957  0.9478\n",
      "     14  83686466.6102  0.9577\n",
      "     15  83593122.8292  0.9568\n",
      "     16  \u001b[36m83490445.8658\u001b[0m  0.9710\n",
      "     17  \u001b[36m83401821.0390\u001b[0m  0.9546\n",
      "     18  \u001b[36m83366228.8012\u001b[0m  1.0488\n",
      "     19  \u001b[36m83332408.2447\u001b[0m  1.2508\n",
      "     20  \u001b[36m83290801.8462\u001b[0m  0.9763\n",
      "     21  \u001b[36m83242503.4123\u001b[0m  0.9708\n",
      "     22  83264213.1113  1.9724\n",
      "     23  \u001b[36m82297966.1676\u001b[0m  0.9587\n",
      "     24  \u001b[36m82134743.5730\u001b[0m  0.9902\n",
      "     25  \u001b[36m82089269.1221\u001b[0m  1.0102\n",
      "     26  \u001b[36m82079435.3833\u001b[0m  1.0656\n",
      "     27  \u001b[36m82074049.0848\u001b[0m  1.0065\n",
      "     28  \u001b[36m82031655.1275\u001b[0m  1.0905\n",
      "     29  82059958.8082  0.9819\n",
      "     30  \u001b[36m82024066.4385\u001b[0m  0.9614\n",
      "     31  82034835.5106  0.9699\n",
      "     32  82080407.2032  0.9864\n",
      "     33  \u001b[36m82012154.9775\u001b[0m  1.0242\n",
      "     34  82053435.1466  1.0471\n",
      "     35  82037304.5025  0.9886\n",
      "     36  82066782.7363  0.9670\n",
      "     37  \u001b[36m82009396.6407\u001b[0m  0.9655\n",
      "     38  \u001b[36m82009037.5937\u001b[0m  0.9567\n",
      "     39  \u001b[36m81990590.1977\u001b[0m  0.9690\n",
      "     40  82035330.8516  0.9583\n",
      "     41  82038695.6820  0.9534\n",
      "     42  \u001b[36m81984509.7804\u001b[0m  0.9548\n",
      "     43  81986868.4021  0.9499\n",
      "     44  82007803.6715  0.9521\n",
      "     45  82010258.7526  0.9492\n",
      "     46  \u001b[36m81979681.6243\u001b[0m  0.9584\n",
      "     47  81983551.6441  0.9588\n",
      "     48  82001524.2529  0.9643\n",
      "     49  \u001b[36m81977950.1930\u001b[0m  0.9566\n",
      "     50  81985082.2608  0.9497\n",
      "     51  \u001b[36m81942120.8453\u001b[0m  0.9561\n",
      "     52  81985915.6771  0.9501\n",
      "     53  81974507.1563  0.9607\n",
      "     54  81947780.6187  0.9674\n",
      "     55  \u001b[36m81941800.9879\u001b[0m  1.0221\n",
      "     56  81966100.4145  1.0524\n",
      "     57  81982008.0804  1.1318\n",
      "     58  \u001b[36m81910459.5029\u001b[0m  1.1148\n",
      "     59  81939400.8173  1.0210\n",
      "     60  81925701.6085  0.9729\n",
      "     61  81959545.4173  0.9679\n",
      "     62  \u001b[36m81890822.9787\u001b[0m  0.9598\n",
      "     63  \u001b[36m81402345.2148\u001b[0m  0.9689\n",
      "     64  \u001b[36m81365439.5867\u001b[0m  0.9583\n",
      "     65  81385189.4937  0.9773\n",
      "     66  \u001b[36m81347672.5670\u001b[0m  926.2177\n",
      "     67  81366484.6767  1.4532\n",
      "     68  \u001b[36m81226812.3702\u001b[0m  0.9610\n",
      "     69  \u001b[36m81151965.2485\u001b[0m  0.9607\n",
      "     70  \u001b[36m81019514.1318\u001b[0m  0.9577\n",
      "     71  \u001b[36m80951760.8141\u001b[0m  0.9623\n",
      "     72  81047761.5460  0.9460\n",
      "     73  81000555.8150  0.9586\n",
      "     74  \u001b[36m80872858.9872\u001b[0m  0.9610\n",
      "     75  \u001b[36m80773149.1935\u001b[0m  0.9664\n",
      "     76  80787926.3422  0.9586\n",
      "     77  \u001b[36m80766049.1031\u001b[0m  0.9626\n",
      "     78  80795983.3041  0.9563\n",
      "     79  \u001b[36m80763828.4101\u001b[0m  0.9646\n",
      "     80  80818443.9607  0.9587\n",
      "     81  80773992.1094  0.9665\n",
      "     82  80784401.2232  0.9641\n",
      "     83  \u001b[36m80738072.5608\u001b[0m  0.9604\n",
      "     84  80785597.0748  0.9592\n",
      "     85  80786585.3887  0.9658\n",
      "     86  80787897.9211  0.9564\n",
      "     87  80784223.9550  0.9601\n",
      "     88  80763372.4270  0.9650\n",
      "     89  80746273.6141  0.9573\n",
      "     90  80797787.1192  0.9592\n",
      "     91  80761356.7594  0.9545\n",
      "     92  80778605.2685  0.9606\n",
      "     93  80760875.8625  0.9472\n",
      "     94  80815757.9307  0.9648\n",
      "     95  80787109.0581  0.9861\n",
      "     96  80798537.1715  0.9997\n",
      "     97  80828275.2874  0.9622\n",
      "     98  \u001b[36m80735547.1120\u001b[0m  0.9559\n",
      "     99  80763820.2572  0.9507\n",
      "    100  80743664.5468  0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m238500686.0357\u001b[0m  0.9907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m78559100.8985\u001b[0m  0.9894\n",
      "      3  \u001b[36m78149494.9901\u001b[0m  0.9881\n",
      "      4  \u001b[36m77946331.4800\u001b[0m  0.9905\n",
      "      5  \u001b[36m77848693.4201\u001b[0m  0.9874\n",
      "      6  \u001b[36m77825739.2070\u001b[0m  0.9915\n",
      "      7  \u001b[36m77811538.1603\u001b[0m  0.9909\n",
      "      8  77942571.8588  0.9810\n",
      "      9  78119182.2331  0.9861\n",
      "     10  78153935.5294  0.9711\n",
      "     11  78351662.8572  0.9829\n",
      "     12  78495713.7722  0.9704\n",
      "     13  78535543.0979  1.0002\n",
      "     14  78408632.7049  1.0567\n",
      "     15  78176251.5521  1.5291\n",
      "     16  78062023.3965  1.3270\n",
      "     17  77897856.2492  1.3020\n",
      "     18  \u001b[36m77772313.4585\u001b[0m  1.0062\n",
      "     19  \u001b[36m77733996.4037\u001b[0m  0.9937\n",
      "     20  \u001b[36m77726864.3616\u001b[0m  0.9873\n",
      "     21  77804381.0926  0.9877\n",
      "     22  77747255.7079  1.0207\n",
      "     23  77822880.2351  0.9909\n",
      "     24  77820576.9872  0.9922\n",
      "     25  77825221.5253  0.9932\n",
      "     26  77808240.9638  0.9884\n",
      "     27  77816425.8671  0.9966\n",
      "     28  77841003.0444  1.0214\n",
      "     29  77782368.1717  1.0146\n",
      "     30  77788276.9573  1.0253\n",
      "     31  77789082.4148  1.0213\n",
      "     32  77805072.8754  1.0213\n",
      "     33  77779031.1394  1.0167\n",
      "     34  77786540.3144  1.0297\n",
      "     35  77776329.1974  1.0292\n",
      "     36  77744891.0967  1.0464\n",
      "     37  77782949.4826  1.0485\n",
      "     38  77756770.4123  1.0315\n",
      "     39  77759763.2863  1.0230\n",
      "     40  77760238.8078  1.0361\n",
      "     41  77783099.6417  1.0406\n",
      "     42  77766808.6566  1.0348\n",
      "     43  77768388.8547  1.0424\n",
      "     44  77783516.5261  1.0468\n",
      "     45  77759031.3016  1.0340\n",
      "     46  77744772.9189  1.0532\n",
      "     47  77766785.9480  1.0539\n",
      "     48  77775043.6881  1.0606\n",
      "     49  77769890.1716  1.0663\n",
      "     50  77749631.2022  1.0497\n",
      "     51  77757106.2709  1.0509\n",
      "     52  77739152.3841  1.0452\n",
      "     53  77754394.4003  1.1229\n",
      "     54  77772891.1617  1.1059\n",
      "     55  77786692.1642  1.0496\n",
      "     56  77731729.7430  1.0453\n",
      "     57  77742097.0116  1.0532\n",
      "     58  77765157.5860  1.0723\n",
      "     59  77758703.6941  1.0666\n",
      "     60  77780268.7705  1.0752\n",
      "     61  77733924.6940  1.0770\n",
      "     62  77732995.2431  1.0632\n",
      "     63  77745189.0024  1.0830\n",
      "     64  77757965.6850  1.0638\n",
      "     65  77738292.4748  1.0771\n",
      "     66  \u001b[36m77721498.6964\u001b[0m  1.0790\n",
      "     67  77768375.1028  1.0850\n",
      "     68  \u001b[36m77713811.4145\u001b[0m  1.0770\n",
      "     69  77789748.4199  1.1443\n",
      "     70  77714788.0111  1.1070\n",
      "     71  \u001b[36m77694463.3463\u001b[0m  1.0951\n",
      "     72  77759080.4656  1.1715\n",
      "     73  77707089.1438  1.1062\n",
      "     74  77753197.6494  1.1034\n",
      "     75  77729032.3654  1.1099\n",
      "     76  \u001b[36m77688061.1014\u001b[0m  1.1145\n",
      "     77  77735592.1234  1.1074\n",
      "     78  77755883.7084  1.1203\n",
      "     79  77733190.9263  1.0942\n",
      "     80  77720348.8344  1.1024\n",
      "     81  77721563.5166  1.1055\n",
      "     82  77735512.3423  1.0865\n",
      "     83  77737094.4445  1.0819\n",
      "     84  77718271.3528  1.1003\n",
      "     85  77702071.2088  1.1291\n",
      "     86  77723581.6102  1.1570\n",
      "     87  77750018.4576  1.1522\n",
      "     88  77714380.3042  1.1468\n",
      "     89  77782991.5946  1.1393\n",
      "     90  77729655.8273  1.1655\n",
      "     91  77723482.7071  1.1403\n",
      "     92  77731177.0763  1.1819\n",
      "     93  77701306.2399  1.1698\n",
      "     94  77696371.7692  1.1887\n",
      "     95  \u001b[36m77685925.6157\u001b[0m  1.1932\n",
      "     96  77781060.8627  1.1766\n",
      "     97  77713640.0174  1.1543\n",
      "     98  77737362.3780  1.1938\n",
      "     99  77712979.0977  1.1592\n",
      "    100  77729053.5214  1.1699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m362806491.8229\u001b[0m  1.1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m82322135.9955\u001b[0m  1.1424\n",
      "      3  82502044.9304  1.1437\n",
      "      4  82469894.6380  1.1707\n",
      "      5  82358838.6336  1.2104\n",
      "      6  \u001b[36m82307718.4250\u001b[0m  1.1811\n",
      "      7  \u001b[36m82251586.8610\u001b[0m  1.1488\n",
      "      8  \u001b[36m82155800.7337\u001b[0m  1.1567\n",
      "      9  82288830.8035  1.1534\n",
      "     10  82422608.7977  1.1709\n",
      "     11  82422769.8884  1.1861\n",
      "     12  82526034.6163  1.1711\n",
      "     13  82591225.3957  1.1742\n",
      "     14  82535366.4282  1.1898\n",
      "     15  82632756.2155  1.2686\n",
      "     16  82516641.6730  1.1788\n",
      "     17  82522883.4641  1.1743\n",
      "     18  82533353.8931  1.1845\n",
      "     19  82324603.7291  1.1922\n",
      "     20  82174180.3830  1.1775\n",
      "     21  82162234.7954  1.2302\n",
      "     22  \u001b[36m82088721.7839\u001b[0m  1.1916\n",
      "     23  82123512.5690  1.1996\n",
      "     24  \u001b[36m82066461.3555\u001b[0m  1.1935\n",
      "     25  82140045.3659  1.1822\n",
      "     26  82097261.7466  1.1773\n",
      "     27  82085039.4776  1.1850\n",
      "     28  82092806.0120  1.1817\n",
      "     29  82078017.7766  1.1797\n",
      "     30  82084860.2691  1.1619\n",
      "     31  \u001b[36m82055574.5101\u001b[0m  1.1951\n",
      "     32  82079683.4035  1.1839\n",
      "     33  \u001b[36m82039444.3632\u001b[0m  1.1770\n",
      "     34  82040942.4264  1.1814\n",
      "     35  \u001b[36m82013904.2512\u001b[0m  1.1636\n",
      "     36  82076200.2524  1.1925\n",
      "     37  82065141.9998  1.1652\n",
      "     38  82059427.5305  1.1818\n",
      "     39  82053395.9971  1.1749\n",
      "     40  82041113.7931  1.2076\n",
      "     41  82022147.0850  1.2216\n",
      "     42  \u001b[36m82012272.1864\u001b[0m  1.2070\n",
      "     43  82028895.1169  1.1972\n",
      "     44  82060414.1856  1.1995\n",
      "     45  \u001b[36m81996413.2648\u001b[0m  1.2126\n",
      "     46  82000107.1204  1.2156\n",
      "     47  82020118.1446  1.2212\n",
      "     48  82004802.9887  1.3281\n",
      "     49  82000500.5046  1.2302\n",
      "     50  \u001b[36m81990726.8408\u001b[0m  1.1905\n",
      "     51  82033107.2414  1.1871\n",
      "     52  \u001b[36m81957923.5488\u001b[0m  1.2096\n",
      "     53  \u001b[36m81942263.2942\u001b[0m  1.2086\n",
      "     54  82011182.6942  1.2428\n",
      "     55  82005860.1701  1.2056\n",
      "     56  \u001b[36m81927164.5604\u001b[0m  1.3140\n",
      "     57  82000944.3941  1.2302\n",
      "     58  81988547.0907  1.2132\n",
      "     59  81969589.2049  1.2234\n",
      "     60  81980648.6960  1.2236\n",
      "     61  81980197.2273  1.2239\n",
      "     62  81960860.6544  1.2323\n",
      "     63  81949832.9398  1.2333\n",
      "     64  81960399.6856  1.2280\n",
      "     65  81997644.7738  1.2332\n",
      "     66  81933658.2019  1.2282\n",
      "     67  82024638.0309  1.2361\n",
      "     68  82042956.8853  1.2418\n",
      "     69  81957108.2075  1.2063\n",
      "     70  \u001b[36m81686854.9670\u001b[0m  1.1890\n",
      "     71  \u001b[36m81425247.2748\u001b[0m  4.2863\n",
      "     72  \u001b[36m81383782.6123\u001b[0m  4.6625\n",
      "     73  \u001b[36m81361991.2914\u001b[0m  4.6604\n",
      "     74  81372535.3292  4.6796\n",
      "     75  81388521.5525  4.7570\n",
      "     76  81366563.2854  4.7025\n",
      "     77  \u001b[36m81361609.0578\u001b[0m  4.6786\n",
      "     78  \u001b[36m81313304.5405\u001b[0m  4.8582\n",
      "     79  81340987.8641  5.0150\n",
      "     80  81350340.1520  4.6803\n",
      "     81  81405248.9781  4.6740\n",
      "     82  81347182.5601  4.6527\n",
      "     83  81335846.0848  4.6765\n",
      "     84  81351062.3560  4.6879\n",
      "     85  81332881.3370  4.6660\n",
      "     86  81337960.4123  4.6652\n",
      "     87  81389824.9449  4.6780\n",
      "     88  81316786.4459  4.6711\n",
      "     89  81341461.4527  4.7938\n",
      "     90  \u001b[36m81234794.2182\u001b[0m  4.7046\n",
      "     91  \u001b[36m80984643.5112\u001b[0m  4.6341\n",
      "     92  81000334.6035  5.1008\n",
      "     93  81088806.9354  4.6794\n",
      "     94  81039671.3721  4.6483\n",
      "     95  \u001b[36m80958104.1261\u001b[0m  4.6609\n",
      "     96  81054767.1537  4.6897\n",
      "     97  80980038.7664  4.6632\n",
      "     98  80976746.6741  4.6804\n",
      "     99  80975664.8446  4.8140\n",
      "    100  \u001b[36m80955086.3072\u001b[0m  4.6619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m282600377.9417\u001b[0m  4.7149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m82641903.0945\u001b[0m  4.6590\n",
      "      3  \u001b[36m82293777.0021\u001b[0m  4.9771\n",
      "      4  \u001b[36m81017189.3462\u001b[0m  4.7804\n",
      "      5  \u001b[36m80662955.9560\u001b[0m  4.6756\n",
      "      6  80723151.7867  4.6912\n",
      "      7  80847194.9529  4.6361\n",
      "      8  80869916.9650  4.6868\n",
      "      9  80751058.1069  4.6739\n",
      "     10  80779184.7623  4.6509\n",
      "     11  80800550.5552  2.6288\n",
      "     12  80700811.9667  1.2353\n",
      "     13  \u001b[36m80615226.2297\u001b[0m  1.2171\n",
      "     14  \u001b[36m80573592.2218\u001b[0m  1.2169\n",
      "     15  \u001b[36m80435133.3902\u001b[0m  1.2289\n",
      "     16  \u001b[36m80250848.2813\u001b[0m  1.2048\n",
      "     17  \u001b[36m80162425.7769\u001b[0m  1.2168\n",
      "     18  \u001b[36m80151844.1704\u001b[0m  1.2206\n",
      "     19  \u001b[36m80119270.8960\u001b[0m  1.2173\n",
      "     20  80147739.1710  1.2889\n",
      "     21  80121374.2278  1.2485\n",
      "     22  \u001b[36m80116321.5145\u001b[0m  1.2042\n",
      "     23  80118649.3155  1.2215\n",
      "     24  \u001b[36m80081768.8045\u001b[0m  1.2179\n",
      "     25  80117672.3795  1.2147\n",
      "     26  80182593.4142  1.2433\n",
      "     27  80117351.8441  1.1941\n",
      "     28  \u001b[36m80052415.1888\u001b[0m  1.2381\n",
      "     29  80098562.8139  1.1916\n",
      "     30  80111274.4797  1.2164\n",
      "     31  80066836.4119  1.2370\n",
      "     32  80083901.8758  1.1965\n",
      "     33  80052793.5828  1.2867\n",
      "     34  \u001b[36m80023479.1995\u001b[0m  1.3069\n",
      "     35  80083705.7547  1.2479\n",
      "     36  80086253.5919  1.2277\n",
      "     37  80073834.1822  1.2267\n",
      "     38  80086789.8700  1.2420\n",
      "     39  80075988.6875  1.9214\n",
      "     40  80026014.6800  4.6197\n",
      "     41  \u001b[36m79989324.3000\u001b[0m  4.6362\n",
      "     42  \u001b[36m79985445.7867\u001b[0m  5.1780\n",
      "     43  80061763.2903  4.6581\n",
      "     44  80014101.7304  4.6493\n",
      "     45  80039310.6628  4.7044\n",
      "     46  \u001b[36m79950458.3184\u001b[0m  4.6600\n",
      "     47  80016997.9408  4.7152\n",
      "     48  80016662.0694  4.8350\n",
      "     49  80010544.9777  4.6669\n",
      "     50  80011827.2806  4.8821\n",
      "     51  79986106.5682  4.6270\n",
      "     52  80022671.3920  4.6305\n",
      "     53  \u001b[36m79926707.7166\u001b[0m  4.6268\n",
      "     54  80007355.2328  4.8248\n",
      "     55  \u001b[36m79910333.6293\u001b[0m  4.6295\n",
      "     56  \u001b[36m79863734.8246\u001b[0m  4.6366\n",
      "     57  79936220.5017  4.6627\n",
      "     58  80007827.8571  4.6353\n",
      "     59  79901286.2369  4.6322\n",
      "     60  79944517.5039  4.6132\n",
      "     61  79937892.1296  4.6852\n",
      "     62  79953861.7757  4.8691\n",
      "     63  79882045.7953  4.7968\n",
      "     64  79925378.4590  4.6573\n",
      "     65  79883765.4094  4.6920\n",
      "     66  79989379.3841  4.5865\n",
      "     67  79915957.5203  4.6417\n",
      "     68  79936516.1555  4.6558\n",
      "     69  \u001b[36m79854014.6381\u001b[0m  4.6610\n",
      "     70  79875758.5287  4.6855\n",
      "     71  79899864.8645  4.6348\n",
      "     72  79917829.3184  4.6235\n",
      "     73  79909370.9044  4.6512\n",
      "     74  79879392.6687  4.6513\n",
      "     75  79879822.3620  4.6384\n",
      "     76  79891762.4714  5.0507\n",
      "     77  79863974.4890  4.7561\n",
      "     78  \u001b[36m79841647.4853\u001b[0m  4.6551\n",
      "     79  \u001b[36m79828442.9154\u001b[0m  4.8796\n",
      "     80  79866534.2395  4.7991\n",
      "     81  79898307.2949  4.6498\n",
      "     82  79889196.0228  4.6725\n",
      "     83  \u001b[36m79790154.3035\u001b[0m  4.6345\n",
      "     84  79838002.0513  4.6424\n",
      "     85  79932506.2815  4.6547\n",
      "     86  79840758.1852  4.6488\n",
      "     87  79812564.4531  4.7155\n",
      "     88  79844721.4642  4.8590\n",
      "     89  79802123.5882  4.8666\n",
      "     90  79821796.6347  4.7303\n",
      "     91  79883827.5352  4.6605\n",
      "     92  79890346.2210  4.6629\n",
      "     93  79871345.9498  4.6149\n",
      "     94  \u001b[36m79776197.5605\u001b[0m  4.6615\n",
      "     95  \u001b[36m79759715.6025\u001b[0m  4.6762\n",
      "     96  79792170.5130  4.6284\n",
      "     97  \u001b[36m79715000.6847\u001b[0m  4.6273\n",
      "     98  \u001b[36m79398463.8072\u001b[0m  4.6486\n",
      "     99  \u001b[36m79255912.5966\u001b[0m  4.6499\n",
      "    100  \u001b[36m79227944.2384\u001b[0m  3.6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m124587544.9949\u001b[0m  1.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m90667983.0996\u001b[0m  1.6569\n",
      "      3  \u001b[36m86859053.7249\u001b[0m  1.7939\n",
      "      4  \u001b[36m85918598.4431\u001b[0m  1.6667\n",
      "      5  85929632.8283  1.6520\n",
      "      6  86035333.8260  3.6322\n",
      "      7  85981825.5250  4.7099\n",
      "      8  85938304.5586  4.6682\n",
      "      9  \u001b[36m84626595.7754\u001b[0m  4.7337\n",
      "     10  \u001b[36m84220425.9074\u001b[0m  4.6645\n",
      "     11  \u001b[36m84034629.7310\u001b[0m  4.6721\n",
      "     12  \u001b[36m83944139.6589\u001b[0m  4.6712\n",
      "     13  \u001b[36m83894669.2908\u001b[0m  4.6521\n",
      "     14  \u001b[36m83824493.3831\u001b[0m  4.6623\n",
      "     15  83844677.7798  4.6619\n",
      "     16  \u001b[36m83812213.0653\u001b[0m  4.6951\n",
      "     17  \u001b[36m83767380.7646\u001b[0m  4.6413\n",
      "     18  83778006.9521  4.8820\n",
      "     19  \u001b[36m83733839.6070\u001b[0m  4.6935\n",
      "     20  \u001b[36m83727681.8805\u001b[0m  4.6708\n",
      "     21  83804806.4771  4.6461\n",
      "     22  83756400.0823  4.9343\n",
      "     23  \u001b[36m83721058.8324\u001b[0m  4.6722\n",
      "     24  \u001b[36m83712763.1256\u001b[0m  4.6854\n",
      "     25  83732519.0354  4.6689\n",
      "     26  83728078.1536  4.7250\n",
      "     27  83727470.0442  4.6809\n",
      "     28  \u001b[36m83682217.3970\u001b[0m  4.6636\n",
      "     29  83693051.4028  4.7641\n",
      "     30  \u001b[36m83653993.2137\u001b[0m  4.9599\n",
      "     31  \u001b[36m83647140.1081\u001b[0m  4.7160\n",
      "     32  83677229.9872  4.7495\n",
      "     33  \u001b[36m83607778.4359\u001b[0m  4.7623\n",
      "     34  83679095.0813  4.7506\n",
      "     35  83632438.0702  4.8042\n",
      "     36  83626110.9764  4.6725\n",
      "     37  83629855.6970  4.6645\n",
      "     38  83638161.4308  4.6714\n",
      "     39  83616686.0925  4.6501\n",
      "     40  83644160.4851  4.6595\n",
      "     41  83640962.3142  4.6476\n",
      "     42  \u001b[36m83607406.0066\u001b[0m  4.6986\n",
      "     43  83620236.1642  4.9345\n",
      "     44  \u001b[36m83567569.3227\u001b[0m  4.6623\n",
      "     45  \u001b[36m83550600.5562\u001b[0m  5.7394\n",
      "     46  83586682.2595  4.8343\n",
      "     47  83585713.1631  4.3129\n",
      "     48  83562608.5077  4.1510\n",
      "     49  \u001b[36m83498979.6726\u001b[0m  4.1589\n",
      "     50  83525337.5918  4.4047\n",
      "     51  83523338.9410  4.2496\n",
      "     52  83593390.0005  4.2053\n",
      "     53  83528564.5351  4.1964\n",
      "     54  \u001b[36m83490406.7248\u001b[0m  4.2476\n",
      "     55  \u001b[36m83484305.6503\u001b[0m  4.3786\n",
      "     56  83549575.5861  4.6248\n",
      "     57  \u001b[36m83446102.7731\u001b[0m  3.6918\n",
      "     58  83498709.1100  3.7074\n",
      "     59  83475622.6678  3.6361\n",
      "     60  83478428.8296  3.6225\n",
      "     61  83515115.2281  3.6882\n",
      "     62  83487792.3856  3.8100\n",
      "     63  83482059.4298  3.7830\n",
      "     64  83480008.5623  3.7384\n",
      "     65  83455562.3950  3.7914\n",
      "     66  \u001b[36m83445614.6324\u001b[0m  3.7701\n",
      "     67  83496305.0875  3.8025\n",
      "     68  \u001b[36m83402741.9675\u001b[0m  3.8560\n",
      "     69  \u001b[36m83382138.3266\u001b[0m  3.8539\n",
      "     70  \u001b[36m82752168.6863\u001b[0m  4.3120\n",
      "     71  \u001b[36m81972754.1615\u001b[0m  4.4968\n",
      "     72  \u001b[36m81704609.1719\u001b[0m  4.0123\n",
      "     73  81711446.0261  3.9087\n",
      "     74  81756757.6959  3.9274\n",
      "     75  81706543.0298  3.9487\n",
      "     76  81730713.6716  3.9435\n",
      "     77  \u001b[36m81677911.1188\u001b[0m  3.9474\n",
      "     78  81734571.1123  3.9449\n",
      "     79  81687131.8048  3.9591\n",
      "     80  81718297.5524  4.0388\n",
      "     81  81725601.6366  4.0071\n",
      "     82  \u001b[36m81654543.1571\u001b[0m  3.9145\n",
      "     83  \u001b[36m81452252.1299\u001b[0m  3.2034\n",
      "     84  \u001b[36m81414562.5541\u001b[0m  3.2579\n",
      "     85  \u001b[36m81375244.9289\u001b[0m  3.7305\n",
      "     86  81395477.7464  3.3833\n",
      "     87  81418304.6899  3.7290\n",
      "     88  81392289.5828  3.8088\n",
      "     89  81397012.0792  3.6875\n",
      "     90  81441990.6564  3.4461\n",
      "     91  81469133.2721  3.8881\n",
      "     92  81399798.4876  3.5313\n",
      "     93  81408725.9941  4.0011\n",
      "     94  81411309.8575  3.6995\n",
      "     95  \u001b[36m81371333.8039\u001b[0m  3.3730\n",
      "     96  81423803.0333  3.4316\n",
      "     97  81420724.8006  3.3845\n",
      "     98  81406278.8436  3.3717\n",
      "     99  \u001b[36m81351994.0953\u001b[0m  3.2311\n",
      "    100  81396592.4988  3.3093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m173432360.4467\u001b[0m  3.3911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m80594432.0902\u001b[0m  3.7592\n",
      "      3  \u001b[36m80585147.9760\u001b[0m  3.2168\n",
      "      4  80615149.1594  3.5601\n",
      "      5  80656621.1891  3.2311\n",
      "      6  80626356.7370  3.5530\n",
      "      7  80638998.0891  3.2619\n",
      "      8  \u001b[36m80573052.0907\u001b[0m  3.1891\n",
      "      9  80702835.2237  3.2449\n",
      "     10  80593111.0940  3.2679\n",
      "     11  \u001b[36m80555563.8690\u001b[0m  3.2994\n",
      "     12  80562783.4014  3.2396\n",
      "     13  \u001b[36m80509824.0922\u001b[0m  3.2321\n",
      "     14  \u001b[36m80355023.6972\u001b[0m  3.2054\n",
      "     15  \u001b[36m79906496.2016\u001b[0m  3.2090\n",
      "     16  \u001b[36m79250438.6488\u001b[0m  3.2860\n",
      "     17  \u001b[36m79081116.5720\u001b[0m  3.2403\n",
      "     18  \u001b[36m79000730.3093\u001b[0m  3.2489\n",
      "     19  \u001b[36m78945048.2954\u001b[0m  3.3121\n",
      "     20  78973361.5888  3.5806\n",
      "     21  \u001b[36m78912254.7963\u001b[0m  3.4886\n",
      "     22  78921347.6870  3.3558\n",
      "     23  78913091.0000  3.2841\n",
      "     24  \u001b[36m78897745.8277\u001b[0m  3.1965\n",
      "     25  78921880.2999  3.5392\n",
      "     26  78992568.5454  3.1967\n",
      "     27  78927459.2935  3.1820\n",
      "     28  \u001b[36m78870987.3283\u001b[0m  3.5274\n",
      "     29  78948121.0654  3.2189\n",
      "     30  78902787.2935  3.3399\n",
      "     31  78913705.3425  3.4830\n",
      "     32  78915672.7191  3.4354\n",
      "     33  78903897.5052  3.3623\n",
      "     34  78883084.4090  3.4150\n",
      "     35  \u001b[36m78833840.0090\u001b[0m  3.6383\n",
      "     36  78950470.6539  3.4013\n",
      "     37  78880842.8072  3.3779\n",
      "     38  78869610.6997  3.5064\n",
      "     39  78902748.2342  3.6770\n",
      "     40  78877132.9017  3.3495\n",
      "     41  78874071.2618  3.2802\n",
      "     42  78871697.2909  3.2468\n",
      "     43  \u001b[36m78815318.7072\u001b[0m  3.3732\n",
      "     44  78882878.3808  3.2228\n",
      "     45  78914221.3467  3.2361\n",
      "     46  78867775.8137  3.2359\n",
      "     47  78841548.6105  3.3687\n",
      "     48  78882397.3194  3.2443\n",
      "     49  78906965.9379  3.2903\n",
      "     50  78839911.1248  3.3030\n",
      "     51  78839821.0162  3.1799\n",
      "     52  78826120.1936  3.2094\n",
      "     53  78838373.5674  3.2805\n",
      "     54  78819214.1649  3.1978\n",
      "     55  78859840.7224  3.3102\n",
      "     56  78866561.1356  3.2356\n",
      "     57  78866692.9288  3.2523\n",
      "     58  78817699.8781  3.5831\n",
      "     59  \u001b[36m78807790.9388\u001b[0m  3.4684\n",
      "     60  78830524.8335  3.2252\n",
      "     61  \u001b[36m78803848.0397\u001b[0m  3.2028\n",
      "     62  78851594.6299  3.1939\n",
      "     63  78855063.9429  3.1888\n",
      "     64  \u001b[36m78771185.4274\u001b[0m  3.2757\n",
      "     65  78794001.9621  3.5379\n",
      "     66  78954421.3684  3.2502\n",
      "     67  78795162.5676  3.2084\n",
      "     68  78946018.7142  3.2615\n",
      "     69  78844090.5186  3.4160\n",
      "     70  78785560.6845  3.1944\n",
      "     71  78883703.3036  3.1826\n",
      "     72  78814980.0696  3.1683\n",
      "     73  78798281.6228  3.2874\n",
      "     74  78804697.7090  3.2316\n",
      "     75  78810259.3637  3.1896\n",
      "     76  78783207.5191  3.3003\n",
      "     77  78874872.3725  3.7289\n",
      "     78  78842820.8105  3.9318\n",
      "     79  \u001b[36m78752760.7821\u001b[0m  3.3235\n",
      "     80  78824775.7952  3.2340\n",
      "     81  78805195.5923  3.2741\n",
      "     82  78809521.2723  3.2080\n",
      "     83  78830811.3868  3.1885\n",
      "     84  78799084.3357  3.3616\n",
      "     85  78760556.8669  3.2409\n",
      "     86  \u001b[36m78748556.1183\u001b[0m  3.3557\n",
      "     87  78776708.6843  3.2619\n",
      "     88  78835786.8824  3.2113\n",
      "     89  78781086.6336  3.2339\n",
      "     90  78752931.6244  3.3307\n",
      "     91  78774590.1713  3.2320\n",
      "     92  \u001b[36m78743038.2334\u001b[0m  3.5498\n",
      "     93  78755990.6419  3.1963\n",
      "     94  78748081.0467  3.5871\n",
      "     95  \u001b[36m78739630.6261\u001b[0m  3.2685\n",
      "     96  78757669.7659  3.3758\n",
      "     97  78750437.0675  3.2023\n",
      "     98  78758102.8209  3.3019\n",
      "     99  78758130.9076  3.2140\n",
      "    100  78768026.0793  3.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch      train_loss     dur\n",
      "-------  --------------  ------\n",
      "      1  \u001b[36m260555528.7793\u001b[0m  3.2194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2  \u001b[36m82936675.8302\u001b[0m  3.2679\n",
      "      3  \u001b[36m82835193.1203\u001b[0m  3.2058\n",
      "      4  82880445.9144  3.2664\n",
      "      5  \u001b[36m82828098.1359\u001b[0m  3.2784\n",
      "      6  \u001b[36m82635220.3547\u001b[0m  3.2516\n",
      "      7  \u001b[36m82625839.0846\u001b[0m  3.3058\n",
      "      8  82763130.8692  3.2476\n",
      "      9  82696105.5724  3.3164\n",
      "     10  82669707.9026  3.1502\n",
      "     11  82794338.2840  3.3430\n",
      "     12  \u001b[36m82622942.7873\u001b[0m  3.6848\n",
      "     13  \u001b[36m82559869.4339\u001b[0m  3.2950\n",
      "     14  \u001b[36m82458357.3516\u001b[0m  3.2599\n",
      "     15  \u001b[36m82310879.2495\u001b[0m  3.2372\n",
      "     16  \u001b[36m82270005.8818\u001b[0m  3.2563\n",
      "     17  \u001b[36m82190925.0913\u001b[0m  3.2659\n",
      "     18  \u001b[36m81964023.3911\u001b[0m  3.3709\n",
      "     19  \u001b[36m81873079.7983\u001b[0m  3.2899\n",
      "     20  \u001b[36m81829755.1966\u001b[0m  3.2897\n",
      "     21  \u001b[36m81817676.9714\u001b[0m  3.4869\n",
      "     22  81888619.9345  3.4067\n",
      "     23  81905443.5394  3.3658\n",
      "     24  81892122.2984  3.3642\n",
      "     25  81902939.0799  3.4051\n",
      "     26  81936561.7144  3.4343\n",
      "     27  81917544.0856  3.3742\n",
      "     28  81913460.9599  3.5779\n",
      "     29  81867918.1748  3.4094\n",
      "     30  81884351.6692  3.5756\n",
      "     31  81893387.2883  3.2878\n",
      "     32  81868329.1198  3.2708\n",
      "     33  81855020.1049  3.2033\n",
      "     34  81855621.8865  3.4087\n",
      "     35  81861018.8602  3.4224\n",
      "     36  81830165.1161  3.1731\n",
      "     37  81873340.3638  3.3435\n",
      "     38  81890951.3140  3.3817\n",
      "     39  81855872.7354  3.3262\n",
      "     40  \u001b[36m81810902.1091\u001b[0m  3.2075\n",
      "     41  81853555.1310  3.2608\n",
      "     42  81844058.1768  3.2087\n",
      "     43  81894933.9667  3.4686\n",
      "     44  81819612.5471  3.2343\n",
      "     45  81850642.0649  3.1786\n",
      "     46  81834855.8011  3.2221\n",
      "     47  81866267.1328  3.2840\n",
      "     48  81861052.2469  3.5746\n",
      "     49  \u001b[36m81807951.7233\u001b[0m  3.1817\n",
      "     50  81839041.1004  3.2433\n",
      "     51  81811950.8837  3.2026\n",
      "     52  81811373.8255  3.3993\n",
      "     53  81829564.3850  3.2742\n",
      "     54  \u001b[36m81798850.1146\u001b[0m  3.4334\n",
      "     55  81800195.1156  3.5170\n",
      "     56  81801886.9656  3.5251\n",
      "     57  \u001b[36m81769564.8716\u001b[0m  3.2413\n",
      "     58  81815619.1601  3.2635\n",
      "     59  81786294.6517  3.2476\n",
      "     60  81837891.7416  3.3762\n",
      "     61  \u001b[36m81742786.4457\u001b[0m  3.2497\n",
      "     62  81778679.9868  3.3465\n",
      "     63  81781800.8299  3.3407\n",
      "     64  81840105.5158  3.2939\n",
      "     65  81769735.6246  3.2080\n",
      "     66  81778027.6711  3.6051\n",
      "     67  \u001b[36m81733833.3981\u001b[0m  3.3186\n",
      "     68  81777452.8205  3.2937\n",
      "     69  81775936.8678  3.1846\n",
      "     70  81764077.9275  3.2672\n",
      "     71  81791909.8052  3.4476\n",
      "     72  81738156.0707  3.2775\n",
      "     73  \u001b[36m81729109.9969\u001b[0m  3.5036\n",
      "     74  81738673.4776  3.3020\n",
      "     75  81730906.5051  3.5343\n",
      "     76  81752217.2372  3.2617\n",
      "     77  81734802.9740  3.3412\n",
      "     78  \u001b[36m81727985.0280\u001b[0m  3.3155\n",
      "     79  81746238.5373  3.2694\n",
      "     80  \u001b[36m81531517.3360\u001b[0m  3.4623\n",
      "     81  \u001b[36m81493380.8287\u001b[0m  3.6240\n",
      "     82  \u001b[36m81450119.9069\u001b[0m  3.2471\n",
      "     83  81453989.8703  3.3117\n",
      "     84  \u001b[36m81406783.6627\u001b[0m  3.8137\n",
      "     85  81460766.3246  3.3992\n",
      "     86  81439969.3086  3.3046\n",
      "     87  81548856.1353  3.2635\n",
      "     88  81560707.8713  3.2733\n",
      "     89  81464500.5535  3.3868\n",
      "     90  81475992.9464  3.3380\n",
      "     91  81436699.4623  3.2337\n",
      "     92  81499872.2073  3.3327\n",
      "     93  81437632.6727  3.4679\n",
      "     94  81468549.6165  3.2029\n",
      "     95  81437013.0601  3.2845\n",
      "     96  81441398.0009  3.2412\n",
      "     97  81461911.3876  3.3156\n",
      "     98  81444571.1265  3.3261\n",
      "     99  \u001b[36m81399631.8449\u001b[0m  3.3287\n",
      "    100  81458674.8224  3.4707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5873.3320\u001b[0m  3.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4832.4637\u001b[0m  3.3748\n",
      "      3     \u001b[36m4807.0160\u001b[0m  3.2694\n",
      "      4     \u001b[36m4806.7239\u001b[0m  3.3466\n",
      "      5     \u001b[36m4802.8719\u001b[0m  3.4078\n",
      "      6     \u001b[36m4799.9021\u001b[0m  3.7147\n",
      "      7     4800.2532  3.2425\n",
      "      8     \u001b[36m4797.5665\u001b[0m  3.2783\n",
      "      9     \u001b[36m4794.1319\u001b[0m  3.3345\n",
      "     10     \u001b[36m4787.5978\u001b[0m  3.5633\n",
      "     11     \u001b[36m4786.5514\u001b[0m  3.3254\n",
      "     12     4786.8147  3.6184\n",
      "     13     \u001b[36m4785.6579\u001b[0m  3.2872\n",
      "     14     4786.8439  3.4347\n",
      "     15     \u001b[36m4782.2406\u001b[0m  3.4432\n",
      "     16     \u001b[36m4764.0276\u001b[0m  3.4920\n",
      "     17     \u001b[36m4650.5521\u001b[0m  4.0280\n",
      "     18     \u001b[36m4647.5934\u001b[0m  3.6608\n",
      "     19     \u001b[36m4646.0982\u001b[0m  3.2219\n",
      "     20     \u001b[36m4644.3892\u001b[0m  3.3862\n",
      "     21     4645.9737  3.4440\n",
      "     22     \u001b[36m4633.8939\u001b[0m  3.3769\n",
      "     23     \u001b[36m4605.6981\u001b[0m  3.3393\n",
      "     24     \u001b[36m4604.0075\u001b[0m  3.3276\n",
      "     25     \u001b[36m4603.3490\u001b[0m  3.2277\n",
      "     26     4603.6520  3.3978\n",
      "     27     4604.3982  3.4360\n",
      "     28     4603.6908  3.2535\n",
      "     29     \u001b[36m4601.2093\u001b[0m  3.2283\n",
      "     30     \u001b[36m4587.4736\u001b[0m  3.4303\n",
      "     31     \u001b[36m4585.3001\u001b[0m  3.3171\n",
      "     32     4587.2730  3.3528\n",
      "     33     \u001b[36m4584.4259\u001b[0m  3.3676\n",
      "     34     4584.7868  3.4548\n",
      "     35     \u001b[36m4583.9526\u001b[0m  3.3246\n",
      "     36     \u001b[36m4583.9417\u001b[0m  3.6232\n",
      "     37     4584.3451  3.2322\n",
      "     38     \u001b[36m4578.8543\u001b[0m  3.3617\n",
      "     39     \u001b[36m4572.1449\u001b[0m  3.4595\n",
      "     40     4578.3619  3.2562\n",
      "     41     \u001b[36m4571.5427\u001b[0m  3.1892\n",
      "     42     4573.7421  3.4124\n",
      "     43     4573.6711  3.3088\n",
      "     44     4576.9899  3.2329\n",
      "     45     4571.7778  3.4523\n",
      "     46     4573.0336  3.3351\n",
      "     47     4573.9459  3.1897\n",
      "     48     4578.2785  3.2938\n",
      "     49     4571.9121  3.4385\n",
      "     50     4572.0779  3.4751\n",
      "     51     4572.1922  4.1783\n",
      "     52     4572.0049  3.2874\n",
      "     53     \u001b[36m4570.8280\u001b[0m  3.4225\n",
      "     54     4573.8099  3.4471\n",
      "     55     4573.5529  3.2593\n",
      "     56     4572.4931  3.2140\n",
      "     57     4576.1463  3.2405\n",
      "     58     4573.2060  3.3312\n",
      "     59     4573.2703  3.5351\n",
      "     60     \u001b[36m4570.1401\u001b[0m  3.3016\n",
      "     61     4570.8879  3.2491\n",
      "     62     4574.1519  3.2144\n",
      "     63     4570.7535  3.4026\n",
      "     64     4575.0903  3.3057\n",
      "     65     4572.1400  3.2683\n",
      "     66     \u001b[36m4569.7634\u001b[0m  3.2175\n",
      "     67     4572.6684  3.3434\n",
      "     68     4572.2591  3.3283\n",
      "     69     4572.4099  3.5326\n",
      "     70     4571.7629  3.1999\n",
      "     71     4571.5427  3.2145\n",
      "     72     4573.3428  3.8767\n",
      "     73     4572.4231  3.3094\n",
      "     74     4572.5250  3.4412\n",
      "     75     4573.1780  3.2858\n",
      "     76     4571.5741  3.2559\n",
      "     77     4571.2388  3.2538\n",
      "     78     4572.2650  3.2152\n",
      "     79     4571.2209  3.2343\n",
      "     80     4570.7196  3.2095\n",
      "     81     4570.9474  3.3074\n",
      "     82     4569.9264  3.2403\n",
      "     83     4572.4373  3.3406\n",
      "     84     \u001b[36m4569.7188\u001b[0m  3.2809\n",
      "     85     \u001b[36m4569.3156\u001b[0m  3.3494\n",
      "     86     4570.2726  3.4047\n",
      "     87     4570.7148  3.2518\n",
      "     88     4571.7323  3.2529\n",
      "     89     4570.6176  3.2313\n",
      "     90     4569.8726  3.7650\n",
      "     91     4573.5966  3.2887\n",
      "     92     4569.9432  3.5132\n",
      "     93     4573.1359  3.2274\n",
      "     94     4569.4693  3.3660\n",
      "     95     4570.1318  3.3239\n",
      "     96     \u001b[36m4569.1949\u001b[0m  3.2306\n",
      "     97     4572.8846  3.1832\n",
      "     98     4569.8066  3.2277\n",
      "     99     4569.9875  3.3451\n",
      "    100     4570.3135  3.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5822.6188\u001b[0m  3.2288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4685.9686\u001b[0m  3.2412\n",
      "      3     \u001b[36m4685.3956\u001b[0m  3.2310\n",
      "      4     \u001b[36m4684.9598\u001b[0m  3.3755\n",
      "      5     \u001b[36m4674.9280\u001b[0m  3.5095\n",
      "      6     \u001b[36m4670.0595\u001b[0m  3.6617\n",
      "      7     \u001b[36m4662.7134\u001b[0m  3.7147\n",
      "      8     \u001b[36m4659.6391\u001b[0m  3.2501\n",
      "      9     \u001b[36m4655.3481\u001b[0m  3.2265\n",
      "     10     \u001b[36m4651.7937\u001b[0m  3.2208\n",
      "     11     4653.9823  3.2519\n",
      "     12     4652.8778  3.2057\n",
      "     13     4652.7380  3.3665\n",
      "     14     4653.2507  3.2644\n",
      "     15     4652.5083  3.2861\n",
      "     16     4652.1754  3.3063\n",
      "     17     4653.6864  3.2924\n",
      "     18     4652.5758  3.2928\n",
      "     19     4652.0374  3.3202\n",
      "     20     4651.9040  3.2480\n",
      "     21     \u001b[36m4651.5157\u001b[0m  3.2107\n",
      "     22     \u001b[36m4651.2917\u001b[0m  3.2907\n",
      "     23     \u001b[36m4650.5941\u001b[0m  3.2826\n",
      "     24     \u001b[36m4650.2202\u001b[0m  3.2374\n",
      "     25     4651.9407  3.6059\n",
      "     26     \u001b[36m4649.3979\u001b[0m  3.2112\n",
      "     27     4649.5421  3.2566\n",
      "     28     \u001b[36m4649.1127\u001b[0m  3.3695\n",
      "     29     4649.6149  3.4117\n",
      "     30     \u001b[36m4648.8676\u001b[0m  3.4191\n",
      "     31     \u001b[36m4648.7405\u001b[0m  3.3241\n",
      "     32     4648.9700  3.3796\n",
      "     33     \u001b[36m4648.5263\u001b[0m  3.3297\n",
      "     34     4648.6473  3.5343\n",
      "     35     4649.1067  3.9897\n",
      "     36     \u001b[36m4648.3336\u001b[0m  3.5435\n",
      "     37     \u001b[36m4647.6240\u001b[0m  3.2718\n",
      "     38     4647.8307  3.2457\n",
      "     39     4650.5194  3.4905\n",
      "     40     \u001b[36m4646.0223\u001b[0m  3.2429\n",
      "     41     4648.0023  3.2974\n",
      "     42     \u001b[36m4645.6264\u001b[0m  3.2272\n",
      "     43     4647.9309  3.3546\n",
      "     44     4646.9530  3.4211\n",
      "     45     4647.0875  3.3715\n",
      "     46     \u001b[36m4645.5028\u001b[0m  3.2187\n",
      "     47     \u001b[36m4636.6089\u001b[0m  3.2482\n",
      "     48     \u001b[36m4633.1432\u001b[0m  3.2288\n",
      "     49     \u001b[36m4619.8061\u001b[0m  3.3202\n",
      "     50     \u001b[36m4617.8744\u001b[0m  3.2472\n",
      "     51     4620.4892  3.4153\n",
      "     52     4618.6715  3.4212\n",
      "     53     4619.6054  3.2422\n",
      "     54     \u001b[36m4617.4039\u001b[0m  3.2299\n",
      "     55     4617.4234  3.2183\n",
      "     56     4617.4404  3.2154\n",
      "     57     4617.8278  3.2696\n",
      "     58     \u001b[36m4616.6347\u001b[0m  3.6838\n",
      "     59     4618.4150  3.2806\n",
      "     60     4616.7289  3.2340\n",
      "     61     \u001b[36m4616.2546\u001b[0m  3.4614\n",
      "     62     4616.5666  3.3805\n",
      "     63     \u001b[36m4611.3598\u001b[0m  3.2297\n",
      "     64     \u001b[36m4609.9320\u001b[0m  3.2443\n",
      "     65     \u001b[36m4609.8478\u001b[0m  3.3111\n",
      "     66     4612.6837  3.2602\n",
      "     67     4610.2828  3.3202\n",
      "     68     4610.4760  3.2331\n",
      "     69     4610.6344  3.2595\n",
      "     70     \u001b[36m4609.6731\u001b[0m  3.2882\n",
      "     71     \u001b[36m4609.2535\u001b[0m  3.2481\n",
      "     72     4610.7929  3.3220\n",
      "     73     4609.8550  3.2620\n",
      "     74     4612.4568  3.2004\n",
      "     75     4609.6092  3.2328\n",
      "     76     4610.4672  3.2881\n",
      "     77     4610.5425  3.3016\n",
      "     78     4610.2250  3.2743\n",
      "     79     \u001b[36m4609.2081\u001b[0m  3.5800\n",
      "     80     4611.1598  3.2537\n",
      "     81     4612.4166  3.3092\n",
      "     82     4609.7943  3.3863\n",
      "     83     4609.2449  3.2993\n",
      "     84     \u001b[36m4608.3617\u001b[0m  3.3194\n",
      "     85     4610.3456  3.2934\n",
      "     86     \u001b[36m4607.9708\u001b[0m  3.3442\n",
      "     87     4608.9990  3.2130\n",
      "     88     4609.4191  3.2433\n",
      "     89     4612.1941  3.2588\n",
      "     90     4609.9386  3.2959\n",
      "     91     4608.7845  3.2736\n",
      "     92     4608.0153  3.2117\n",
      "     93     4608.8449  3.2390\n",
      "     94     4610.8922  3.3879\n",
      "     95     4609.6556  3.2897\n",
      "     96     4611.8539  3.7446\n",
      "     97     4608.4676  3.6024\n",
      "     98     4609.0961  3.3473\n",
      "     99     4608.9884  3.3455\n",
      "    100     4608.1234  3.2254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5438.9869\u001b[0m  3.2987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4582.8305\u001b[0m  3.2693\n",
      "      3     \u001b[36m4579.1324\u001b[0m  3.4950\n",
      "      4     4580.3221  3.3082\n",
      "      5     \u001b[36m4578.0574\u001b[0m  3.3870\n",
      "      6     \u001b[36m4571.5447\u001b[0m  3.2597\n",
      "      7     \u001b[36m4567.2938\u001b[0m  3.2527\n",
      "      8     4569.1187  3.2786\n",
      "      9     \u001b[36m4564.5675\u001b[0m  3.2840\n",
      "     10     4567.7029  3.2738\n",
      "     11     4567.0613  3.3006\n",
      "     12     4565.2654  3.2330\n",
      "     13     4566.0572  3.2088\n",
      "     14     4566.2434  3.8891\n",
      "     15     4566.8657  3.2920\n",
      "     16     4566.5133  3.3018\n",
      "     17     4565.0527  3.2432\n",
      "     18     4565.7613  3.2704\n",
      "     19     \u001b[36m4562.9638\u001b[0m  3.2805\n",
      "     20     \u001b[36m4562.7491\u001b[0m  3.2511\n",
      "     21     \u001b[36m4561.4243\u001b[0m  3.2614\n",
      "     22     4563.3080  3.2628\n",
      "     23     4564.2917  3.3202\n",
      "     24     4562.7318  3.2614\n",
      "     25     4564.2094  3.2486\n",
      "     26     \u001b[36m4560.1299\u001b[0m  3.2011\n",
      "     27     4564.1550  3.2433\n",
      "     28     4561.5514  3.2627\n",
      "     29     4562.8244  3.3733\n",
      "     30     4561.4487  3.2155\n",
      "     31     4563.6193  3.2298\n",
      "     32     4564.1190  3.5827\n",
      "     33     4563.9474  3.2793\n",
      "     34     4562.4749  3.2471\n",
      "     35     4563.0714  3.2743\n",
      "     36     4561.0478  3.3157\n",
      "     37     4562.8672  3.2404\n",
      "     38     4562.1915  3.2634\n",
      "     39     4562.4776  3.2875\n",
      "     40     4560.8444  3.3240\n",
      "     41     \u001b[36m4558.9545\u001b[0m  3.3231\n",
      "     42     4561.9736  3.2880\n",
      "     43     4562.5025  3.2586\n",
      "     44     4562.5547  3.3353\n",
      "     45     4560.2106  3.2855\n",
      "     46     4562.6303  3.2447\n",
      "     47     4561.9732  3.2782\n",
      "     48     4560.2267  3.8741\n",
      "     49     4562.8910  3.3290\n",
      "     50     4562.5579  3.6134\n",
      "     51     4560.3778  3.3699\n",
      "     52     4563.6067  3.2491\n",
      "     53     4559.4744  3.2582\n",
      "     54     4563.3537  3.2360\n",
      "     55     4559.6514  3.2874\n",
      "     56     4560.1748  3.5167\n",
      "     57     4562.9578  3.2673\n",
      "     58     4561.1902  3.3404\n",
      "     59     4561.1261  3.2526\n",
      "     60     \u001b[36m4557.8871\u001b[0m  3.2169\n",
      "     61     4562.0520  3.2198\n",
      "     62     4561.8370  3.2471\n",
      "     63     4565.9816  3.3119\n",
      "     64     4566.4778  3.2136\n",
      "     65     4562.0472  3.3114\n",
      "     66     4561.0546  3.3176\n",
      "     67     4564.7822  3.2844\n",
      "     68     4560.8538  3.2470\n",
      "     69     4565.0869  3.5385\n",
      "     70     4561.5612  3.3079\n",
      "     71     4563.6243  3.4352\n",
      "     72     4561.1149  3.1987\n",
      "     73     4559.5475  3.2711\n",
      "     74     4563.8904  3.3434\n",
      "     75     4564.7978  3.2498\n",
      "     76     4563.5608  3.2315\n",
      "     77     4561.5759  3.2287\n",
      "     78     4559.2029  3.3622\n",
      "     79     4559.9099  3.6179\n",
      "     80     4561.0858  3.3712\n",
      "     81     4561.9793  3.2896\n",
      "     82     4563.0878  3.2626\n",
      "     83     4561.4401  3.3081\n",
      "     84     4559.9745  3.2597\n",
      "     85     4561.1887  3.2647\n",
      "     86     4562.5297  3.2782\n",
      "     87     4562.5435  3.5941\n",
      "     88     4560.1423  3.2803\n",
      "     89     4558.6929  3.3616\n",
      "     90     4561.5906  3.2979\n",
      "     91     4559.5560  3.2163\n",
      "     92     4559.8624  3.2693\n",
      "     93     4560.9168  3.3499\n",
      "     94     4560.9809  3.6826\n",
      "     95     4561.0272  3.5133\n",
      "     96     4560.2186  3.2450\n",
      "     97     4562.1115  3.2661\n",
      "     98     4567.9055  3.2689\n",
      "     99     \u001b[36m4557.7245\u001b[0m  3.2367\n",
      "    100     \u001b[36m4555.4640\u001b[0m  3.3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6831.0055\u001b[0m  3.3390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m5934.2432\u001b[0m  3.2460\n",
      "      3     \u001b[36m5928.2492\u001b[0m  3.1924\n",
      "      4     \u001b[36m5922.2634\u001b[0m  3.4911\n",
      "      5     \u001b[36m5916.2823\u001b[0m  3.2069\n",
      "      6     \u001b[36m5910.3042\u001b[0m  3.2190\n",
      "      7     \u001b[36m5904.3290\u001b[0m  3.1659\n",
      "      8     \u001b[36m5898.3552\u001b[0m  3.2111\n",
      "      9     \u001b[36m5892.3870\u001b[0m  3.1975\n",
      "     10     \u001b[36m5886.4327\u001b[0m  3.2861\n",
      "     11     \u001b[36m5880.4786\u001b[0m  3.2233\n",
      "     12     \u001b[36m5877.4123\u001b[0m  3.2935\n",
      "     13     \u001b[36m5874.7825\u001b[0m  3.3049\n",
      "     14     \u001b[36m5862.6446\u001b[0m  3.1725\n",
      "     15     \u001b[36m5856.6969\u001b[0m  3.1600\n",
      "     16     \u001b[36m5850.7503\u001b[0m  3.2682\n",
      "     17     \u001b[36m5845.3498\u001b[0m  3.1900\n",
      "     18     \u001b[36m5838.9200\u001b[0m  3.1908\n",
      "     19     \u001b[36m5833.0294\u001b[0m  3.6420\n",
      "     20     \u001b[36m5827.2478\u001b[0m  3.1665\n",
      "     21     5828.7452  3.1504\n",
      "     22     \u001b[36m4932.8915\u001b[0m  3.7107\n",
      "     23     \u001b[36m4669.7647\u001b[0m  3.3220\n",
      "     24     \u001b[36m4662.8830\u001b[0m  3.2233\n",
      "     25     \u001b[36m4653.6989\u001b[0m  3.2160\n",
      "     26     \u001b[36m4636.3366\u001b[0m  3.2707\n",
      "     27     \u001b[36m4631.7769\u001b[0m  3.2129\n",
      "     28     \u001b[36m4622.0512\u001b[0m  3.2192\n",
      "     29     \u001b[36m4615.1653\u001b[0m  3.2222\n",
      "     30     \u001b[36m4612.7902\u001b[0m  3.2279\n",
      "     31     \u001b[36m4611.6186\u001b[0m  3.2875\n",
      "     32     \u001b[36m4611.0044\u001b[0m  3.2611\n",
      "     33     \u001b[36m4605.7086\u001b[0m  3.2288\n",
      "     34     4608.1247  3.2103\n",
      "     35     4606.5722  3.2607\n",
      "     36     \u001b[36m4604.7154\u001b[0m  3.2074\n",
      "     37     \u001b[36m4601.4918\u001b[0m  3.3157\n",
      "     38     4604.1119  3.7496\n",
      "     39     4603.7201  3.2087\n",
      "     40     4603.2566  3.6769\n",
      "     41     4605.5917  3.2422\n",
      "     42     4603.2345  3.2101\n",
      "     43     \u001b[36m4599.6454\u001b[0m  3.1950\n",
      "     44     4603.6410  3.2417\n",
      "     45     4604.3447  3.1987\n",
      "     46     4603.7816  3.2854\n",
      "     47     4610.4573  3.2334\n",
      "     48     \u001b[36m4595.8278\u001b[0m  3.2468\n",
      "     49     4596.0421  3.3344\n",
      "     50     4598.2847  3.3113\n",
      "     51     4597.3982  3.2765\n",
      "     52     4598.5150  3.2115\n",
      "     53     4597.3342  3.2115\n",
      "     54     4597.0678  3.2071\n",
      "     55     4597.6983  3.2585\n",
      "     56     4599.2931  3.3006\n",
      "     57     4597.3971  3.2124\n",
      "     58     4596.3453  3.2153\n",
      "     59     4598.4116  3.5560\n",
      "     60     4596.7797  3.1868\n",
      "     61     4598.9014  3.2485\n",
      "     62     4597.4355  3.3191\n",
      "     63     4597.2576  3.2071\n",
      "     64     4598.3509  3.2103\n",
      "     65     4596.6633  3.2370\n",
      "     66     \u001b[36m4595.7857\u001b[0m  3.4272\n",
      "     67     4599.6794  3.6043\n",
      "     68     4596.6902  3.2161\n",
      "     69     4595.7878  3.2179\n",
      "     70     4596.9785  3.1911\n",
      "     71     4596.9674  3.2665\n",
      "     72     4596.1668  3.3019\n",
      "     73     4596.7772  3.1785\n",
      "     74     4597.0449  3.2512\n",
      "     75     4596.8965  3.4001\n",
      "     76     4598.7690  3.7902\n",
      "     77     4598.3910  3.6661\n",
      "     78     4596.9494  3.3009\n",
      "     79     4596.2597  3.3025\n",
      "     80     \u001b[36m4594.2807\u001b[0m  3.2284\n",
      "     81     4594.3888  3.2086\n",
      "     82     4594.6299  3.2619\n",
      "     83     \u001b[36m4594.0365\u001b[0m  3.3373\n",
      "     84     4594.6989  3.2125\n",
      "     85     4597.9716  3.2068\n",
      "     86     4595.9664  3.2148\n",
      "     87     \u001b[36m4593.9250\u001b[0m  3.2124\n",
      "     88     4594.4791  3.2980\n",
      "     89     4594.0038  3.2974\n",
      "     90     \u001b[36m4593.8127\u001b[0m  3.2132\n",
      "     91     \u001b[36m4593.7755\u001b[0m  3.2386\n",
      "     92     4594.0086  3.2977\n",
      "     93     \u001b[36m4593.0112\u001b[0m  3.3454\n",
      "     94     4594.4574  3.3752\n",
      "     95     4594.0396  3.6475\n",
      "     96     4593.3493  3.2336\n",
      "     97     \u001b[36m4592.5469\u001b[0m  4.3659\n",
      "     98     4596.2875  3.4319\n",
      "     99     4594.6112  3.2945\n",
      "    100     4594.2001  3.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6008.7281\u001b[0m  3.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4970.3594\u001b[0m  3.1956\n",
      "      3     \u001b[36m4945.6813\u001b[0m  3.3083\n",
      "      4     \u001b[36m4942.8392\u001b[0m  3.1898\n",
      "      5     \u001b[36m4937.7983\u001b[0m  3.2505\n",
      "      6     \u001b[36m4934.2528\u001b[0m  3.1958\n",
      "      7     \u001b[36m4932.0558\u001b[0m  3.1883\n",
      "      8     \u001b[36m4929.2861\u001b[0m  3.3265\n",
      "      9     \u001b[36m4924.6354\u001b[0m  3.2729\n",
      "     10     \u001b[36m4924.1962\u001b[0m  3.2500\n",
      "     11     \u001b[36m4918.3271\u001b[0m  3.2272\n",
      "     12     \u001b[36m4827.8695\u001b[0m  3.6040\n",
      "     13     \u001b[36m4729.0894\u001b[0m  3.3389\n",
      "     14     \u001b[36m4728.0292\u001b[0m  3.2174\n",
      "     15     \u001b[36m4727.9599\u001b[0m  3.1960\n",
      "     16     \u001b[36m4725.6269\u001b[0m  3.1987\n",
      "     17     \u001b[36m4724.6175\u001b[0m  3.2776\n",
      "     18     \u001b[36m4723.5535\u001b[0m  3.2175\n",
      "     19     \u001b[36m4723.3956\u001b[0m  3.2422\n",
      "     20     4724.5156  3.1743\n",
      "     21     \u001b[36m4722.1158\u001b[0m  3.2105\n",
      "     22     \u001b[36m4721.6755\u001b[0m  3.2547\n",
      "     23     \u001b[36m4719.6161\u001b[0m  3.2200\n",
      "     24     \u001b[36m4698.6723\u001b[0m  3.2861\n",
      "     25     \u001b[36m4658.9521\u001b[0m  3.3390\n",
      "     26     \u001b[36m4656.5456\u001b[0m  3.2290\n",
      "     27     \u001b[36m4655.5078\u001b[0m  3.3029\n",
      "     28     4656.9709  3.5220\n",
      "     29     4656.1509  3.3008\n",
      "     30     \u001b[36m4653.9142\u001b[0m  3.1995\n",
      "     31     4658.1099  3.8196\n",
      "     32     \u001b[36m4651.7562\u001b[0m  3.4091\n",
      "     33     4652.9211  3.3708\n",
      "     34     \u001b[36m4639.6010\u001b[0m  3.5183\n",
      "     35     \u001b[36m4622.6870\u001b[0m  3.2209\n",
      "     36     4623.9143  3.2534\n",
      "     37     4623.2738  3.2766\n",
      "     38     \u001b[36m4613.5051\u001b[0m  3.2261\n",
      "     39     \u001b[36m4606.5269\u001b[0m  3.3532\n",
      "     40     \u001b[36m4594.7229\u001b[0m  3.2884\n",
      "     41     4595.8432  4.1038\n",
      "     42     \u001b[36m4588.8705\u001b[0m  3.4293\n",
      "     43     \u001b[36m4580.7282\u001b[0m  3.2076\n",
      "     44     4582.3039  3.2868\n",
      "     45     \u001b[36m4580.6904\u001b[0m  3.2846\n",
      "     46     4581.2625  3.3102\n",
      "     47     4585.4668  3.2344\n",
      "     48     4583.7105  3.4987\n",
      "     49     4582.7660  3.2720\n",
      "     50     4582.2442  3.2523\n",
      "     51     4583.8676  3.2706\n",
      "     52     4583.4500  3.3008\n",
      "     53     4588.8718  3.2602\n",
      "     54     4581.3402  3.2834\n",
      "     55     4584.6197  3.2020\n",
      "     56     4584.2549  3.2245\n",
      "     57     4584.8162  3.3331\n",
      "     58     4585.1981  3.2723\n",
      "     59     4584.8691  3.2263\n",
      "     60     4584.4620  3.2448\n",
      "     61     4583.6708  3.2439\n",
      "     62     4583.4919  3.2417\n",
      "     63     4581.4370  3.2640\n",
      "     64     4583.9388  3.2136\n",
      "     65     4584.0966  3.7367\n",
      "     66     4584.6778  3.5110\n",
      "     67     4584.7208  3.3560\n",
      "     68     4583.7851  3.2769\n",
      "     69     4581.7905  3.3205\n",
      "     70     4583.5708  3.3114\n",
      "     71     4584.3103  3.2605\n",
      "     72     4585.0846  3.2447\n",
      "     73     4587.0670  3.2204\n",
      "     74     4583.5022  3.3860\n",
      "     75     4584.1678  3.2166\n",
      "     76     4582.9596  3.3200\n",
      "     77     4583.5595  3.2238\n",
      "     78     4584.0019  3.3166\n",
      "     79     4582.8599  3.2371\n",
      "     80     4583.1717  3.3939\n",
      "     81     4582.7936  3.2931\n",
      "     82     4582.0532  3.4926\n",
      "     83     4582.6174  3.2444\n",
      "     84     4580.7674  3.2545\n",
      "     85     4581.5185  3.5962\n",
      "     86     4583.4983  3.2290\n",
      "     87     4585.2354  3.2682\n",
      "     88     \u001b[36m4579.6527\u001b[0m  3.2555\n",
      "     89     4581.7186  3.2389\n",
      "     90     4582.5081  3.2497\n",
      "     91     4582.4086  3.2233\n",
      "     92     4584.3112  3.3326\n",
      "     93     4581.1393  3.2316\n",
      "     94     4580.8508  3.4089\n",
      "     95     4583.1565  3.3821\n",
      "     96     \u001b[36m4578.7503\u001b[0m  3.2991\n",
      "     97     4581.4337  3.2556\n",
      "     98     4584.5557  3.1904\n",
      "     99     4581.5967  3.2396\n",
      "    100     4580.2082  3.4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m6974.6177\u001b[0m  3.1674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m5934.2589\u001b[0m  3.5729\n",
      "      3     \u001b[36m5928.2585\u001b[0m  3.1986\n",
      "      4     \u001b[36m5922.2697\u001b[0m  3.2249\n",
      "      5     \u001b[36m5916.2870\u001b[0m  3.1749\n",
      "      6     \u001b[36m5910.3084\u001b[0m  3.2173\n",
      "      7     \u001b[36m5904.3326\u001b[0m  3.2388\n",
      "      8     \u001b[36m5898.3587\u001b[0m  3.2492\n",
      "      9     \u001b[36m5892.3903\u001b[0m  3.2509\n",
      "     10     \u001b[36m5886.4359\u001b[0m  3.4228\n",
      "     11     \u001b[36m5880.4818\u001b[0m  3.2615\n",
      "     12     \u001b[36m5874.5276\u001b[0m  3.2180\n",
      "     13     \u001b[36m5868.5744\u001b[0m  3.1980\n",
      "     14     \u001b[36m5862.6242\u001b[0m  3.2444\n",
      "     15     \u001b[36m5856.6789\u001b[0m  3.2465\n",
      "     16     \u001b[36m5850.7337\u001b[0m  3.3120\n",
      "     17     \u001b[36m5844.7921\u001b[0m  3.2774\n",
      "     18     \u001b[36m5838.8963\u001b[0m  4.4058\n",
      "     19     \u001b[36m5833.0069\u001b[0m  3.2323\n",
      "     20     \u001b[36m5827.1111\u001b[0m  3.6354\n",
      "     21     5834.7411  3.2096\n",
      "     22     \u001b[36m5720.0086\u001b[0m  3.1843\n",
      "     23     \u001b[36m4666.6823\u001b[0m  3.2345\n",
      "     24     \u001b[36m4645.4270\u001b[0m  3.2225\n",
      "     25     \u001b[36m4640.4079\u001b[0m  3.4627\n",
      "     26     \u001b[36m4639.8577\u001b[0m  3.3143\n",
      "     27     \u001b[36m4639.0815\u001b[0m  3.3908\n",
      "     28     \u001b[36m4637.9332\u001b[0m  3.3610\n",
      "     29     4639.3104  3.4170\n",
      "     30     4638.3145  3.4485\n",
      "     31     4640.1511  3.2584\n",
      "     32     4639.8819  3.3604\n",
      "     33     4638.2830  3.2587\n",
      "     34     \u001b[36m4636.9364\u001b[0m  3.2595\n",
      "     35     4639.4207  3.4287\n",
      "     36     4639.2378  3.3264\n",
      "     37     4639.0509  3.2585\n",
      "     38     \u001b[36m4635.8483\u001b[0m  3.6157\n",
      "     39     4637.2017  3.2367\n",
      "     40     4639.5781  3.0035\n",
      "     41     4636.9190  27.9376\n",
      "     42     \u001b[36m4633.7775\u001b[0m  2.4693\n",
      "     43     \u001b[36m4631.5941\u001b[0m  0.9972\n",
      "     44     4631.6111  0.9836\n",
      "     45     4632.4412  1.0011\n",
      "     46     4632.4982  0.9892\n",
      "     47     \u001b[36m4630.7296\u001b[0m  0.9854\n",
      "     48     4631.6115  0.9910\n",
      "     49     4631.9560  0.9852\n",
      "     50     \u001b[36m4630.6400\u001b[0m  0.9725\n",
      "     51     \u001b[36m4628.7620\u001b[0m  0.9868\n",
      "     52     \u001b[36m4627.3046\u001b[0m  0.9759\n",
      "     53     \u001b[36m4625.6302\u001b[0m  0.9844\n",
      "     54     \u001b[36m4624.9762\u001b[0m  0.9864\n",
      "     55     4625.8507  0.9873\n",
      "     56     4626.0972  0.9899\n",
      "     57     4625.6094  0.9821\n",
      "     58     4626.0659  0.9772\n",
      "     59     \u001b[36m4624.2040\u001b[0m  0.9882\n",
      "     60     \u001b[36m4623.5799\u001b[0m  0.9940\n",
      "     61     4626.4493  0.9801\n",
      "     62     4625.4759  0.9936\n",
      "     63     4625.4988  0.9939\n",
      "     64     4627.3367  0.9873\n",
      "     65     4624.8796  0.9780\n",
      "     66     4626.1630  0.9925\n",
      "     67     \u001b[36m4623.0572\u001b[0m  0.9815\n",
      "     68     4625.4704  0.9984\n",
      "     69     4624.7217  0.9925\n",
      "     70     4624.8514  0.9980\n",
      "     71     4624.5613  1.0404\n",
      "     72     4624.6098  0.9887\n",
      "     73     4624.3704  0.9811\n",
      "     74     4623.3298  0.9818\n",
      "     75     4625.2952  0.9861\n",
      "     76     \u001b[36m4622.4648\u001b[0m  0.9834\n",
      "     77     4623.9162  0.9788\n",
      "     78     4624.1831  1.0163\n",
      "     79     \u001b[36m4622.2123\u001b[0m  1.0246\n",
      "     80     4623.3105  1.0205\n",
      "     81     4624.4726  1.0174\n",
      "     82     4624.4339  1.0227\n",
      "     83     4623.5422  1.0488\n",
      "     84     4623.9200  1.0181\n",
      "     85     4625.1600  1.1252\n",
      "     86     4623.7887  926.6306\n",
      "     87     4623.8475  0.9877\n",
      "     88     4623.2655  1.0230\n",
      "     89     4625.2322  1.0334\n",
      "     90     4624.2395  0.9885\n",
      "     91     4623.6800  0.9857\n",
      "     92     4623.4013  0.9844\n",
      "     93     4623.9308  0.9850\n",
      "     94     \u001b[36m4620.5875\u001b[0m  0.9892\n",
      "     95     4624.7744  0.9842\n",
      "     96     4623.0800  0.9849\n",
      "     97     4622.9222  0.9968\n",
      "     98     4623.4435  0.9762\n",
      "     99     4622.5057  0.9920\n",
      "    100     4623.4592  1.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5824.2724\u001b[0m  0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([245])) that is different to the input size (torch.Size([245, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4733.5711\u001b[0m  0.9780\n",
      "      3     \u001b[36m4651.5612\u001b[0m  0.9807\n",
      "      4     \u001b[36m4644.5510\u001b[0m  0.9897\n",
      "      5     \u001b[36m4643.8503\u001b[0m  0.9840\n",
      "      6     4645.4181  1.0255\n",
      "      7     \u001b[36m4640.0401\u001b[0m  1.0322\n",
      "      8     \u001b[36m4629.0451\u001b[0m  1.1267\n",
      "      9     \u001b[36m4617.1841\u001b[0m  1.0072\n",
      "     10     \u001b[36m4613.7799\u001b[0m  1.0064\n",
      "     11     \u001b[36m4612.7562\u001b[0m  0.9949\n",
      "     12     \u001b[36m4612.4704\u001b[0m  1.0051\n",
      "     13     \u001b[36m4612.2731\u001b[0m  0.9834\n",
      "     14     4613.2730  0.9876\n",
      "     15     4613.4615  0.9809\n",
      "     16     \u001b[36m4611.9526\u001b[0m  0.9947\n",
      "     17     4612.0685  0.9912\n",
      "     18     \u001b[36m4610.8101\u001b[0m  0.9851\n",
      "     19     \u001b[36m4610.5146\u001b[0m  0.9900\n",
      "     20     4613.8129  0.9857\n",
      "     21     \u001b[36m4609.6234\u001b[0m  0.9823\n",
      "     22     4612.8030  0.9870\n",
      "     23     4610.7155  0.9873\n",
      "     24     4612.1302  0.9790\n",
      "     25     4610.3256  0.9923\n",
      "     26     4612.1537  0.9810\n",
      "     27     4611.6752  0.9776\n",
      "     28     \u001b[36m4607.8791\u001b[0m  0.9758\n",
      "     29     4611.8876  0.9817\n",
      "     30     4609.3624  1.0097\n",
      "     31     4610.4061  0.9821\n",
      "     32     4608.6256  924.0402\n",
      "     33     4610.4135  1.0410\n",
      "     34     \u001b[36m4607.3762\u001b[0m  0.9765\n",
      "     35     4609.0684  0.9873\n",
      "     36     \u001b[36m4606.9116\u001b[0m  1.0050\n",
      "     37     4611.0483  0.9773\n",
      "     38     4608.0314  0.9870\n",
      "     39     \u001b[36m4597.7601\u001b[0m  0.9794\n",
      "     40     \u001b[36m4595.3897\u001b[0m  0.9880\n",
      "     41     \u001b[36m4587.7450\u001b[0m  0.9914\n",
      "     42     \u001b[36m4584.3608\u001b[0m  0.9883\n",
      "     43     4584.9797  0.9839\n",
      "     44     4588.3371  0.9865\n",
      "     45     4586.6821  0.9724\n",
      "     46     \u001b[36m4583.5663\u001b[0m  0.9829\n",
      "     47     4584.5518  1.0103\n",
      "     48     \u001b[36m4581.0193\u001b[0m  1.0271\n",
      "     49     4582.4871  0.9756\n",
      "     50     \u001b[36m4577.8442\u001b[0m  1.0003\n",
      "     51     4580.8400  0.9779\n",
      "     52     4578.0911  0.9847\n",
      "     53     4584.8086  0.9963\n",
      "     54     4581.2650  0.9936\n",
      "     55     \u001b[36m4577.7416\u001b[0m  1.0032\n",
      "     56     4578.2407  0.9929\n",
      "     57     4579.8605  0.9984\n",
      "     58     4580.2070  0.9873\n",
      "     59     4586.5978  0.9912\n",
      "     60     4585.7065  1.0120\n",
      "     61     4583.9823  1.0063\n",
      "     62     4585.3891  1.1976\n",
      "     63     \u001b[36m4577.5493\u001b[0m  1.1977\n",
      "     64     4578.3740  1.1726\n",
      "     65     \u001b[36m4577.5369\u001b[0m  1.0418\n",
      "     66     \u001b[36m4577.5048\u001b[0m  1.0106\n",
      "     67     4577.5750  1.0303\n",
      "     68     4579.0911  1.0251\n",
      "     69     \u001b[36m4576.7202\u001b[0m  1.0236\n",
      "     70     4578.7215  1.0348\n",
      "     71     \u001b[36m4576.4244\u001b[0m  1.0108\n",
      "     72     \u001b[36m4576.2641\u001b[0m  1.0167\n",
      "     73     4578.6606  1.0192\n",
      "     74     4576.6606  1.0171\n",
      "     75     4581.0440  1.0185\n",
      "     76     4577.9083  1.0295\n",
      "     77     4578.7074  1.0279\n",
      "     78     \u001b[36m4576.0903\u001b[0m  0.9988\n",
      "     79     4576.6202  1.0143\n",
      "     80     4577.5891  1.0091\n",
      "     81     \u001b[36m4576.0569\u001b[0m  1.0237\n",
      "     82     4577.1093  1.0193\n",
      "     83     4577.0629  1.0308\n",
      "     84     \u001b[36m4575.3335\u001b[0m  1.0096\n",
      "     85     4577.6204  1.0228\n",
      "     86     4575.8489  1.0370\n",
      "     87     \u001b[36m4574.7486\u001b[0m  1.0097\n",
      "     88     4574.9762  1.0288\n",
      "     89     \u001b[36m4573.2451\u001b[0m  1.0348\n",
      "     90     \u001b[36m4572.2249\u001b[0m  1.0172\n",
      "     91     4576.8234  1.0329\n",
      "     92     4578.0431  1.0051\n",
      "     93     4575.0710  1.0303\n",
      "     94     \u001b[36m4572.1200\u001b[0m  1.0009\n",
      "     95     4572.9395  1.0353\n",
      "     96     \u001b[36m4571.1019\u001b[0m  1.0385\n",
      "     97     4576.1217  1.0613\n",
      "     98     4573.0721  1.0674\n",
      "     99     4573.5681  1.0487\n",
      "    100     \u001b[36m4570.0473\u001b[0m  1.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5614.4765\u001b[0m  1.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([246])) that is different to the input size (torch.Size([246, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4676.5999\u001b[0m  1.0487\n",
      "      3     \u001b[36m4664.8443\u001b[0m  1.0718\n",
      "      4     \u001b[36m4664.6712\u001b[0m  1.0650\n",
      "      5     \u001b[36m4659.7718\u001b[0m  1.1819\n",
      "      6     \u001b[36m4655.9442\u001b[0m  1.0819\n",
      "      7     \u001b[36m4648.1613\u001b[0m  1.0753\n",
      "      8     \u001b[36m4639.2966\u001b[0m  1.1013\n",
      "      9     \u001b[36m4631.9318\u001b[0m  1.0687\n",
      "     10     \u001b[36m4623.2526\u001b[0m  1.0683\n",
      "     11     \u001b[36m4620.5280\u001b[0m  1.0904\n",
      "     12     \u001b[36m4620.1860\u001b[0m  1.1071\n",
      "     13     4622.9269  1.0796\n",
      "     14     4621.8595  1.0894\n",
      "     15     4632.2865  1.0796\n",
      "     16     4631.3999  1.0655\n",
      "     17     4630.4294  1.0576\n",
      "     18     4630.4158  1.0831\n",
      "     19     4628.9889  1.0748\n",
      "     20     4628.8875  1.0819\n",
      "     21     4629.1730  1.0818\n",
      "     22     4629.2209  1.1014\n",
      "     23     4629.2269  1.0925\n",
      "     24     4630.3780  1.0901\n",
      "     25     4629.3360  1.1074\n",
      "     26     4629.9512  1.1135\n",
      "     27     4628.8079  1.1175\n",
      "     28     4628.7224  1.1050\n",
      "     29     4627.4028  1.1113\n",
      "     30     4629.2668  1.1160\n",
      "     31     4627.8716  1.1147\n",
      "     32     4628.8968  1.1042\n",
      "     33     4627.4716  1.1173\n",
      "     34     4628.4568  1.1121\n",
      "     35     4629.0889  1.1386\n",
      "     36     4629.2391  1.1426\n",
      "     37     4627.1952  1.1599\n",
      "     38     4627.3016  1.1659\n",
      "     39     4628.7144  1.1340\n",
      "     40     4628.3144  1.1585\n",
      "     41     4629.2215  1.1384\n",
      "     42     4629.5225  1.1416\n",
      "     43     4629.9313  1.1592\n",
      "     44     4626.3941  1.1595\n",
      "     45     4625.8958  1.1615\n",
      "     46     4628.0723  1.1615\n",
      "     47     4629.2437  1.1322\n",
      "     48     4624.4957  1.1855\n",
      "     49     4629.0420  1.1803\n",
      "     50     4629.3042  1.1960\n",
      "     51     4628.2793  1.3004\n",
      "     52     4627.1990  1.1295\n",
      "     53     4628.5220  1.1321\n",
      "     54     4628.6902  1.1341\n",
      "     55     4625.9242  1.1677\n",
      "     56     4627.0937  1.1738\n",
      "     57     4625.0071  1.2231\n",
      "     58     4627.2821  1.2601\n",
      "     59     4625.5578  1.1910\n",
      "     60     4628.3767  1.1876\n",
      "     61     4625.0782  1.1996\n",
      "     62     4628.3558  1.1834\n",
      "     63     4626.4901  1.2162\n",
      "     64     4626.3579  1.1961\n",
      "     65     4625.4130  1.1990\n",
      "     66     4623.2078  1.1849\n",
      "     67     4621.0327  1.2129\n",
      "     68     4620.5938  1.1835\n",
      "     69     \u001b[36m4619.9156\u001b[0m  1.1518\n",
      "     70     4622.7276  1.1797\n",
      "     71     4622.0691  1.2187\n",
      "     72     \u001b[36m4617.1862\u001b[0m  1.1863\n",
      "     73     4621.8438  1.1953\n",
      "     74     4618.1268  1.2063\n",
      "     75     4617.3913  1.2136\n",
      "     76     4617.5711  1.2025\n",
      "     77     \u001b[36m4615.6773\u001b[0m  1.2069\n",
      "     78     4617.2626  1.1775\n",
      "     79     4617.0043  1.1884\n",
      "     80     4616.5434  1.1920\n",
      "     81     4618.0166  1.1964\n",
      "     82     4618.6780  1.1990\n",
      "     83     4619.2473  1.4117\n",
      "     84     4617.3842  1.4853\n",
      "     85     4617.5084  1.2272\n",
      "     86     4617.4590  1.2105\n",
      "     87     4616.0966  1.2258\n",
      "     88     4616.1616  1.2095\n",
      "     89     4622.1418  1.2068\n",
      "     90     4616.7940  1.2319\n",
      "     91     4616.0344  1.2412\n",
      "     92     4619.9186  1.2139\n",
      "     93     4618.1887  1.2196\n",
      "     94     \u001b[36m4615.2823\u001b[0m  1.2332\n",
      "     95     4616.8173  1.2236\n",
      "     96     \u001b[36m4615.1765\u001b[0m  1.2225\n",
      "     97     4615.8870  1.2076\n",
      "     98     4615.8391  1.2196\n",
      "     99     4618.3348  1.2176\n",
      "    100     4615.9875  1.1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([300])) that is different to the input size (torch.Size([300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m5656.7894\u001b[0m  2.4801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelarruda/miniconda3/envs/deep-learning-deAaZ-Pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([191])) that is different to the input size (torch.Size([191, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2     \u001b[36m4970.8629\u001b[0m  2.4344\n",
      "      3     \u001b[36m4964.5731\u001b[0m  2.4774\n",
      "      4     \u001b[36m4960.7350\u001b[0m  2.5396\n",
      "      5     \u001b[36m4959.4025\u001b[0m  2.4632\n",
      "      6     \u001b[36m4948.6961\u001b[0m  2.5116\n",
      "      7     \u001b[36m4939.3836\u001b[0m  2.5099\n",
      "      8     \u001b[36m4935.2077\u001b[0m  2.4832\n",
      "      9     \u001b[36m4931.1619\u001b[0m  2.5212\n",
      "     10     \u001b[36m4928.6503\u001b[0m  2.5466\n",
      "     11     \u001b[36m4925.5078\u001b[0m  2.5441\n",
      "     12     \u001b[36m4827.4229\u001b[0m  2.5462\n",
      "     13     \u001b[36m4741.6071\u001b[0m  2.5269\n",
      "     14     \u001b[36m4653.0090\u001b[0m  2.5563\n",
      "     15     \u001b[36m4646.3910\u001b[0m  2.5935\n",
      "     16     \u001b[36m4646.0684\u001b[0m  2.6732\n",
      "     17     \u001b[36m4641.1955\u001b[0m  4.8533\n",
      "     18     \u001b[36m4627.7832\u001b[0m  10.5701\n",
      "     19     4628.2246  11.2933\n",
      "     20     \u001b[36m4624.7153\u001b[0m  9.8442\n",
      "     21     4625.4717  9.6350\n",
      "     22     4625.7221  9.5970\n",
      "     23     4626.8758  9.5469\n",
      "     24     \u001b[36m4615.8012\u001b[0m  9.7445\n",
      "     25     \u001b[36m4614.7555\u001b[0m  9.8747\n",
      "     26     \u001b[36m4612.7348\u001b[0m  10.0853\n",
      "     27     4614.2810  9.7130\n",
      "     28     \u001b[36m4610.5714\u001b[0m  9.8229\n",
      "     29     4616.2979  9.4832\n",
      "     30     4613.9663  9.5465\n",
      "     31     4615.3838  9.6117\n",
      "     32     4615.2330  10.0984\n",
      "     33     4614.3384  6.4574\n",
      "     34     4616.3446  2.4699\n",
      "     35     \u001b[36m4608.1126\u001b[0m  2.4757\n",
      "     36     \u001b[36m4602.7892\u001b[0m  2.5962\n",
      "     37     \u001b[36m4602.0800\u001b[0m  2.4417\n",
      "     38     4603.8274  2.4567\n",
      "     39     4602.1819  2.4175\n",
      "     40     4602.7844  2.4204\n",
      "     41     4602.1673  2.4480\n",
      "     42     \u001b[36m4600.9121\u001b[0m  2.4350\n",
      "     43     4602.1028  2.4557\n",
      "     44     4603.0467  2.4683\n",
      "     45     4602.2804  2.4770\n",
      "     46     4601.7533  2.4686\n",
      "     47     4601.2926  2.4956\n",
      "     48     4601.1304  2.5384\n",
      "     49     \u001b[36m4600.5281\u001b[0m  2.5578\n",
      "     50     \u001b[36m4600.0493\u001b[0m  2.5026\n",
      "     51     \u001b[36m4598.5031\u001b[0m  2.5129\n",
      "     52     \u001b[36m4596.2292\u001b[0m  2.5463\n",
      "     53     4598.4728  5.2921\n",
      "     54     \u001b[36m4595.0797\u001b[0m  9.5379\n",
      "     55     4596.8052  9.4963\n",
      "     56     4597.0653  9.4771\n",
      "     57     4596.2845  9.7493\n",
      "     58     4597.1313  9.5078\n",
      "     59     4595.6355  9.7099\n",
      "     60     4596.6371  9.4975\n",
      "     61     4595.9952  9.6340\n",
      "     62     4596.1229  9.5268\n",
      "     63     4597.1658  9.5382\n",
      "     64     4596.4298  9.4581\n",
      "     65     4596.3382  9.5618\n",
      "     66     4596.2558  9.6627\n",
      "     67     4597.6213  9.4522\n",
      "     68     4596.8469  3.9784\n",
      "     69     4597.2581  2.8906\n",
      "     70     4595.5196  2.8563\n",
      "     71     4596.3525  2.8998\n",
      "     72     4595.3156  2.8676\n",
      "     73     4595.3592  2.9113\n",
      "     74     4595.1192  2.8937\n",
      "     75     4595.6219  2.9174\n",
      "     76     \u001b[36m4594.2323\u001b[0m  2.9238\n",
      "     77     4596.1698  2.9430\n",
      "     78     4595.5385  2.9164\n",
      "     79     4594.8774  2.9779\n",
      "     80     4596.4449  2.9630\n",
      "     81     4595.6069  4.4803\n",
      "     82     4595.2191  9.4969\n",
      "     83     4595.1343  9.5381\n",
      "     84     4595.7724  9.7106\n",
      "     85     4594.9256  9.4367\n",
      "     86     4594.7842  9.5453\n",
      "     87     4595.6563  9.8684\n",
      "     88     4595.9538  9.7172\n",
      "     89     4594.4977  9.5438\n",
      "     90     4597.2163  9.4859\n",
      "     91     4595.1742  9.4849\n",
      "     92     4595.4948  9.5080\n",
      "     93     4596.0888  9.6625\n",
      "     94     4594.9987  9.7645\n",
      "     95     4597.7472  9.6027\n",
      "     96     \u001b[36m4593.6265\u001b[0m  9.6074\n",
      "     97     4596.4785  9.5196\n",
      "     98     4594.8709  9.8343\n",
      "     99     4594.0829  9.4966\n",
      "    100     4595.1904  9.7003\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(previsores, preco_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b453493-d93e-4709-8ec4-238f1a7e48a6",
   "metadata": {},
   "source": [
    "## 6. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41edc9c4-f2ce-46a3-b7ba-774293d52d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_mae = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10bba74a-a2eb-4588-9fbe-0ecb8a37da01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 300,\n",
       " 'criterion': torch.nn.modules.loss.L1Loss,\n",
       " 'max_epochs': 100,\n",
       " 'module__activation': <function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
       " 'module__dropout': 0.3,\n",
       " 'module__initializer': <function torch.nn.init.normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, generator: Optional[torch._C.Generator] = None) -> torch.Tensor>,\n",
       " 'module__neurons': 158,\n",
       " 'optimizer': torch.optim.adam.Adam}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2e4b509-1bb4-459d-baac-d09a087b09ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-4529.0771484375)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99be5d-0308-4770-8544-0dbafaa245af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
